\chapter{System Overview}

\section{Model Architecture}\label{sec:model_arch}
\subsection{hgru4rec}
In section~\ref{sec:hgru4rec} we introduced the model architecture for hgru4rec.
This work proposes and improvement of this model by using the model introduced in~\ref{sec:meta_prod2vec}.
As mentioned before hgru4rec uses a one-hot encoding of items as the input, as seen in equation~\ref{eq:hgru4rec_session}.
We propose to use pre-computed embeddings resulting from Meta-Prod2Vec as a replacement for the one-hot encodings.
Therefore the model architecture is not different from the one seen in figure~\ref{fig:hgru4rec}.
Meta-Prod2Vec is implemented and trained independently of the session-based model.
After training Meta-Prod2Vec, the product embeddings are extracted and saved to a key-value store.
Afterwards hgru4rec can access this key-value store during training and inference such that the model can consume the product embeddings instead of the one-hot vectors.
\paragraph{Model training}
After testing different optimizers the Adam optimizer performed best, therefore we used this for training the hierarchical RNN.
Each model was trained until there was no more improvement in the validation metric.

\subsection{Meta-Prod2Vec}
In section~\ref{sec:meta_prod2vec} we introduced the model architecture for Meta-Prod2Vec.
The model has a rather simple architecture, consisting of a single embedding layer, which is fitted using the loss function shown in equation~\ref{eq:meta_prod2vec_loss}
The following side-information was used:
\begin{itemize}
    \item Brand
    \item Product Type
    \item Price Class
\end{itemize}
The product type specifies which category a product is in, for example "Mobile Phone" or "Dining Table".
The mapping from products to brands and product types is very simple, since these are two properties each product must have.
The price class is computed within a product type.
The idea is to classify products into a a number of classes, such that the model can learn to fit "premium" products closer together.
To achieve that we extract a number of quantiles from all the prices of the products within a certain product type.
Then we assign IDs to the different quantiles, and this ID is then attached to the product as an additional side information.
Afterwards the product and metadata space are joined, by creating and embedding dictionary that maps all the entities from the product and metadata space to a single contiuous ID space.
This is done since the input to the projection layer is a one-hot encoding of the entity, since obviously we are again dealing with categorical data.
As we have seen in equation~\ref{eq:meta_prod2vec_loss} the loss function would support a different weighting of the different types of side-information, however we found that this does not help the model learn better embeddings, therefore we use equal weights for all the types of side-information.
\paragraph{Model training}
The model is simply trained over two epochs.
This decision was made because first the second pass over the data decreases the loss significantly, whereas the third pass does not.

\section{Implementation}
\subsection{hgru4rec}
The session based model was implemented in Tensorflow\footnote{\url{http://tensorflow.org}}.
In figure~\ref{fig:hgru4rec_implementation} we can see an illustration of the different components of the code.

\subsection{Meta-Prod2Vec}
\begin{itemize}
    \item we could make a diagram showing the different components
\item Describe Class Diagram
\item Describe prediction mode/training mode
\item Describe problems that arose during training (extensive resources used for so many products and users)
\end{itemize}
\section{API}\label{sec:api}
As described above the model is implemented in Tensorflow.
Tensorflow provides a mechanism to export tensorflow models as so called SavedModel\footnote{\url{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md}}, which is the way Tensorflow serializes models universally.
They also provide a premade Docker image\footnote{\url{https://hub.docker.com/r/tensorflow/serving}} which allows the model to be served as a REST or GRPC API.
\begin{itemize}
\item Describe API
\end{itemize}

\section{Production Setup}
Digitec Galaxus AG is the largest online-retailer in Switzerland.
They operate galaxus.ch and digitec.ch. The former is a general online-shop comparable to Amazon. 
The latter is specialized in Electronics.
Distributed on the different sections of the site there are several recommendation engines populating the content the users see.
Examples are the landing page\todo{Add screenshot} and multiple engines on the product detail page\todo{Add screenshot}.
% At the time of writing these engines were either recommending products or marketing pages. 
% \par
% Marketing Pages are a part of Digitec Galaxus' marketing strategy.
% There is a editorial team which is separated by a chinese wall from the product departement.
% This team writes independant reviews and other stories including current news.
% These pages give the customers a second reason to interact with the platform, by establishing themselves as a news platform for users.
% Therefore it makes sense to invest in recommendations of articles, since this allows the user to be familiar with the platform, and later preferring it for an online purchase.
% These pages are not considered in this work, however the model should be applicable to marketing pages as well. 

% \subsection{Survival of the Fittest Framework}
% As mentioned above there are multiple locations, such as the product detail page, in which recommendation engines can display content.
% Moreover in each of these locations is it possible to have multiple spots which display content from recommendation engines.
% For example on the product detail page there are multple spots displaying recommendations.
% The Survival of the Fittest Framework is a system designed to decide which recommendation engine runs in which spot. \todo{Show an illustration for the framework}
% The framework is a way of tackling the exploration exploitation tradeoff.
% The definition which recommendation engine is allowed to be displayed in which spot is done manually, since not all combinations of the two make sense from a user experience perspective.
% In principle popular recommendation engines get a proportionally higher probability to be selected when such a page is accessed by a specific user, while maintaining some restrictions.
% This probability of being selected is defined as follows:\todo{Add specific formula}
% \[
%     p_{xij} = max(0.05, )
% \]
% Where $p_{xij}$ is the probability of recommendation engine $x$ being displayed on location $i$ and spot $j$.
% This probability is computed constantly, automatically based on a stream of user clicks.
% The Click-Through-Rate is the relevant metric for this computation because as described in~\ref{conversion_rate} this can only be estimated from the sales, since the true intent of the user cannot be reliably identified.
% Therefore less popular or new recommendation engines still can be selected to be displayed, allowing for exploration of different approaches.
% Should an engine become popular due to an improvement in the logic behind it, the probability of display increases automatically as more users interact with this element.

There is a framework that computes probabilities which specific recommendation engine provides the content for a specific location.
Further the framework then chooses the content for each location based on the probabilities computed before and some other constraints such as minimum and maximum value.
However to test the model implemented in this work this framework is bypassed by a A/B Testing engine, therefore this framework is not part of this work.
The specific tests and the test setup is described in~\ref{sec:exp_setup}, for the understanding of the following it is enough to assume that some independant system is providing recommendation requests to the recommendation system.
Using the API containers described in~\ref{sec:api} we can serve these requests.
The sequence-diagram in figure~\ref{fig:serving_recs} should give an overview on how the system is integrated in the production environment.

\begin{figure}[H]
	\centering
	\captionsetup{width=0.8\textwidth}
    \includegraphics[width=\textwidth]{serving-recommendations.png}
    \caption{Serving Recommendations on digitec.ch and galaxus.ch}
    \label{fig:serving_recs}
\end{figure}

The whole process starts when a specific user requests a specific page on either digitec.ch or galaxus.ch.
If the requested page has an element where there is an A/B test configured the Online-Shop Application will make a request to the testing engine.
As mentioned above the testing engine will then make a request to a API location, which corresponds to one of the model versions.
After that the Online Shop will request the content, in this case, from the Personalization Infrastructure.
Note that during this setup each of the four versions of the model is deployed simultaneously, all of them trained on the same dataset.
The Personalization Infrastructure then calls the API described above.
Before requesting a prediction from tf-serve the API will gather precomputed embeddings for the involved entities.
The predictions are returned to the Personalization Infrastructure and from there to the Online Shop.
The last step is for the Online Shop to render the content and deliver the page to the user.
This process takes about 300ms.