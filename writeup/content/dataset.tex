\chapter{Dataset}

\section{Data Collection}
In this work we will use the data generated by the tracking systems of Digitec Galaxus AG.
The following sequence-diagram gives an overview of the data collection process.

\begin{figure}[H]
	\centering
	\captionsetup{width=0.8\textwidth}
    \includegraphics[width=\textwidth]{collecting-data.png}
    \caption{Collecting user interaction data on digitec.ch and galaxus.ch}
    \label{fig:collecting_data}
\end{figure}

When a user requests a specific page, the Online-Shop Application will collect some information on the user such as UserId, requested page, User Agent etc.
This information is packaged as a message representing this specific event and then sent to a message queue.
After that the Personalization Infrastructure will process these events by requesting batches of unprocessed messages.
For each event then the Personalization Infrastructure will do two things: First it will update the involved entities, second it will archive the event.
In the former case a managed Key-Value store (Google Cloud Datastore) is used to store entities.
Examples of such entities are Shopping Carts, Orders, and Last Viewed Products.
In principle these entities represent the current state of various entities appearing in the context of the Online-Shop.
In the latter case a managed Data Warehousing solution (Google BigQuery) is used, this solution provides the possibility to store large amounts of data in append-only tables.
The data stored there is mainly denormalized, such that easy extraction is possible.
Each row in these append-only tables contains all information belonging to a single event produced by a user.
Essentially the data stored in the Key-Value store is the sum of all events stored in the data warehouse.

\section{Data Preparation}
To be able to focus on the model implementation when implementing the model we want to prepare the data for congestion as far as possible.
Therefore the extraction and preparation of the dataset is implemented separately from the model implementation.
The process is very similar to the process described in~\cite{hierarchical}.
\paragraph{Data Extraction}
In the first step we extract the raw data from the data warehouse, selecting the following pieces of information: ProductId, LastLoggedInUserId, UserId, SessionId, User Agent, Timestamp.
Most of the properties are self-explanatory, except LastLoggedInUserId.
This property represents the UserId last seen on a specific device accessing the Online-Shop, therefore it is useful if the user did not actively log in to the account.
In this case we would not know which user accessed the page, however using this property we can complete missing UserIds in the dataset.
The data extracted is limited to the events producted by visiting a product detail page as seen in figure~\ref{fig:product_detail}.
This filtering is done because this work focuses on recommending products, in another setting other data might also be relevant.
The extracted data is stored in several shards of CSV files, each shard approximately represents the events of one day.

\begin{figure}[ht]
	\centering
	\captionsetup{width=0.8\textwidth}
    \includegraphics[width=\textwidth]{product-detail.png}
    \caption{Product Detail Page on galaxus.ch}
    \label{fig:product_detail}
\end{figure}

\paragraph{Cleaning data}
In the next step the data is cleaned, which involves mainly two steps.
Anytime we encounter an event where we do not know the user, we try to complete the information with the LastLoggedInUserId. 
If we do not know the LastLoggedInUserId as well we discard the event. \\
There are many well-known bots roaming the Internet collecting various types of information on websites.
Most famously the Google Bot which crawls the content of any website to determine the quality of the site and influence the rank of the website in Google searches.
There are some open-source lists that collect the User Agents of these bots.
The User Agent of a client accessing a website usually describes the type of device used, in the case of "good" bots they explicitly tell the server that the "device" accessing the website is actually a bot.
Using one of these lists\footnote{\url{https://raw.githubusercontent.com/monperrus/crawler-user-agents/master/crawler-user-agents.json}} the events produced by bots are removed from the dataset.
\paragraph{Aggregation of events}
The next step is to aggregate the events on two levels, first grouping events that were generated in the same session and second grouping these sessions by users.
The resulting data structure looks as follows:

\begin{minipage}{\linewidth}
    \begin{lstlisting}[language=Python,frame=single,caption=Data Structure for user-events,label=code:user-events]
    "<UserId_1>": {
        "<SessionId_1>": {
            "StartTime": "<Timestamp of first event>",
            "Events": [
                {
                    "ProductId": "<ProductId_1>",
                    "Timestamp": "<Timestamp_1>"
                },
                {
                    "ProductId": "<ProductId_2>",
                    "Timestamp": "<Timestamp_2>"
                }
            ]
        }
    }
    \end{lstlisting}
\end{minipage}
This is done by processing one shard after another, when a shard is finished the JSON representation of this shard is saved in a separate file.
The reason is that the large amount of datapoints cannot be kept in memory altogether.

\paragraph{Merging shards}
Since the shards only approximate the events of one day, it is not possible to assume that a session is completely represented in one shard, it might be distributed across multiple.
Therefore it is necessary to merge the data in the different shards, however since it is not efficient to keep all the information in one file, a different method of paritioning is needed.
However as we will see later it is further necessary that all the events produced by a single user are in the same file.
An easy way of achieving this is by partitioning by UserId.
Each shard is processed as follows:

\begin{minipage}{\linewidth}
    \begin{lstlisting}[language=Python,frame=single,caption=Merging shards,label=code:merging-shards]
    for shard in shards:
        for i in range(num_target_files):
            relevant_user_ids = list(filter(lambda x: int(x) % num_target_files == i, shard.keys()))
            output_path = merged_shards_prefix + str(i) + '.json'
            output_file = json.load(output_path)
            for user_id in relevant_user_ids:
                for session_id in shard[user_id]:
                    # Merge events from output_file and shard
    \end{lstlisting}
\end{minipage}

\paragraph{Filtering}
Now we have some number of files containing all the information relevant to some users.
As the authors of~\ref{hierarchical} mentioned as well, it makes sense to filter out some datapoints.
Specifically the following:
\begin{itemize}
    \item Remove items with low support (min 5 Events per product)
    \item Remove users with few sessions (min 5 Sessions per user)
    \item Remove sessions with few events (min 3 Events per session)
\end{itemize}
The reasons for removing these is rather obvious.
Items with few interactions are not optimal for modeling, sessions that are too short are not really informative, and finally users with few sessions produce few cross-session information.

\paragraph{Subsampling}
Prototyping models with very large datasets is very inefficient.
Therefore several different sizes of the dataset were produced.
First the approximate number of users and the exact number of products desired in the dataset is defined.
This was done by sampling from the set of products with a probability proportional to the number of events on that product.
In a next step the partitions of the data are processed, iterating through the sessions of the users.
For each sessions we remove the products that were not chosen to be kept, if the session is still long enough, the session is kept.
Further a user is kept in the dataset if the number of filtered sessions is still large enough.
This process is repeated for the different partitions until the number of users is larger than the approximate number of desired users.

\paragraph{Embedding Dictionary}
As we will see in~\ref{model_arch} a version of the model uses one-hot encodings for representing products.
Therefore the product IDs referenced in the dataset need to be mapped into a continuous ID space, otherwise it is not possible to produce one-hot encodings.
The process for this is straight forward: Start with EmbeddingId 0 then iterate through all the sessions, and each time a product is seen for the first time it will be assigned the EmbeddingId and the EmbeddingId is increased by 1.

\section{User Parallel Batches}
Since we are dealing with sequence data, it is not trivial to produce batches for the model to ingest.
Especially since the sequences can have different lengths, and the number of sessions varies from user to user.
User Parallel Batching is a way of having fixed size batches that can be ingested by the model, while allowing for sequences to have different lengths and users to have a different number of sessions.
Usually in these cases the sequences are padded with some neutral value, however in this case there is no neutral product, and the introduction of such a neutral product might influence the results.
Furthermore when processing sequence data it usually means that a datapoint is the input for the next datapoint, which means we still need to have the ordered sequence data.
To illustrate these batches lets assume that there are 4 users with the sessions as in figure~\ref{fig:user_parallel_batches}.

\begin{figure}[ht]
	\centering
	\captionsetup{width=0.8\textwidth}
    \includegraphics[width=\textwidth]{user_parallel_batches.png}
    \caption{User Parallel Batches}
    \label{fig:user_parallel_batches}
\end{figure}

In principle a batch of size $x$ will contain an event from $x$ different users.
Essentially the labels are the event that happens after the input event.
\\
The data representation shown in listing~\ref{code:user-events} is chosen explicitly to enable efficient generation of user parallel batches.

\section{Dataset properties}\label{sec:dataset_properties}
\todo{Show dataset stats for different datasets}