\chapter{Dataset}

\section{Data Collection}
\subsection{Data Generation Mechanism}
\begin{itemize}
    \item Each click the user makes on the site is tracked
    \item An event is generated containing the information and context of the click of the user
    \item This event is sent to a message queue
    \item From the message queue there is a subscriber that archives this event in Google BigQuery
    \item The most important fields tracked are: UserId, SessionId, ProductId, Timestamp, LastLoggedInUserId
    \item Google BigQuery is a Data Warehousing solution consisting of append-only tables
\end{itemize}
\subsection{Data Storage}
\begin{itemize}
\item Data is stored in BigQuery in a denormalized form
\item One row describes one event
\end{itemize}

\section{Data Extraction}
\begin{itemize}
\item First we extract the events on ProductDetail Pages from BigQuery
\item This is done in shards, where each shard approximately corresponds to one day of events
\item We only use the data where we know which user produced the event
\item The events are filtered first by removing all events generated by known bot user agents
\item In a second step we merge the fields UserId and LastLoggedInUserId, if the latter is set we know that the user "passively" logged out of the shop (long time without a session)
\item Then the events are grouped by sessions
\item Then the sessions are grouped by users
\item After that we filter out users that have too few sessions
\item Then we filter products that have too few events
\item After that we have a dataset that contains all users with enough sessions and all events on products with enough events
\item Show the structure of one user in the json
\end{itemize}

\section{User Parallel Batches}
\begin{itemize}
    \item Show how the user parallel batches should look like
    \item Explain how we transform the json dataset to these user parallel batches
\end{itemize}
