{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from hgru4rec.user_par_mini_batch import input_fn, UserParallelMiniBatchDataset\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnGRU\n",
    "from tensorflow.keras.layers import GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['num_units_session'] = 25\n",
    "params['num_units_user'] = 50\n",
    "params['num_products'] = 2600000\n",
    "params['embedding_size'] = 25\n",
    "params['user_rnn_layers'] = 2\n",
    "params['user_rnn_units'] = 50\n",
    "params['session_rnn_layers'] = 2\n",
    "params['session_rnn_units'] = 25\n",
    "params['num_negative_samples'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/muy/.local/share/virtualenvs/code-JIyQgSPM/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /home/muy/.local/share/virtualenvs/code-JIyQgSPM/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# get datapoint iterator\n",
    "dataset = input_fn(10, 'gs://ma-muy/baseline_dataset/sessions_by_user/', 3, epochs=2)\n",
    "datapoints = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muy/.local/share/virtualenvs/code-JIyQgSPM/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/home/muy/.local/share/virtualenvs/code-JIyQgSPM/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'UserId': <tf.Tensor: id=254, shape=(10,), dtype=int64, numpy=\n",
       " array([1721500,  582800,  898100, 2241000, 2206200, 2802000, 2159700,\n",
       "        1624500, 1897400,  834800])>,\n",
       " 'ProductId': <tf.Tensor: id=252, shape=(10,), dtype=int64, numpy=\n",
       " array([ 399224, 5949616, 5882871, 9669627, 6012120, 6353176, 7665009,\n",
       "         432765, 6032131, 6168248])>,\n",
       " 'EmbeddingId': <tf.Tensor: id=251, shape=(10,), dtype=int64, numpy=array([10, 11, 12,  3, 13,  5, 14, 15,  8, 16])>,\n",
       " 'UserEmbeddingId': <tf.Tensor: id=253, shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoint = next(datapoints)\n",
    "features, labels = datapoint\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State representation\n",
    "\n",
    "batch_size = features['UserId'].shape[0]\n",
    "\n",
    "ended_sessions_mask = tf.get_variable(\n",
    "        'ended_sessions_mask',\n",
    "        shape=(batch_size,),\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        trainable=False,\n",
    "        dtype=tf.bool)\n",
    "\n",
    "ending_sessions_mask = tf.get_variable(\n",
    "        'continuing_sessions_mask',\n",
    "        shape=(batch_size,),\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        trainable=False,\n",
    "        dtype=tf.bool)\n",
    "\n",
    "ended_users_mask = tf.get_variable(\n",
    "        'ended_users_mask',\n",
    "        shape=(batch_size,),\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        trainable=False,\n",
    "        dtype=tf.bool)\n",
    "\n",
    "session_hidden_states = tf.get_variable(\n",
    "        'session_hidden_states',\n",
    "        shape=(batch_size, params['session_rnn_units']),\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "user_hidden_states = tf.get_variable(\n",
    "        'user_hidden_states',\n",
    "        shape=(batch_size, params['user_rnn_units']),\n",
    "        initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "\n",
    "embeddings = tf.get_variable(\n",
    "    'embedding',\n",
    "    shape=(params['num_products'], params['embedding_size']))\n",
    "\n",
    "softmax_weights = tf.get_variable(\n",
    "    'softmax_weights',\n",
    "    shape=(params['num_products'], params['embedding_size']))\n",
    "\n",
    "softmax_biases = tf.get_variable(\n",
    "    'softmax_biases',\n",
    "    shape=(params['num_products'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User & Session RNN\n",
    "\n",
    "user_rnn = GRU(\n",
    "    # params['user_rnn_layers'],\n",
    "    params['user_rnn_units'],\n",
    "    return_state=True,\n",
    "    name='user_rnn')\n",
    "\n",
    "session_rnn = GRU(\n",
    "    # params['session_rnn_layers'],\n",
    "    params['session_rnn_units'],\n",
    "    return_state=True,\n",
    "    name='session_rnn')\n",
    "\n",
    "user2session_layer = Dense(params['session_rnn_units'], input_shape=(params['user_rnn_units'],), activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Session & User Representation for ended users\n",
    "\n",
    "session_hidden_states = tf.where(\n",
    "    ended_users_mask,\n",
    "    tf.zeros(tf.shape(session_hidden_states)),\n",
    "    session_hidden_states)\n",
    "\n",
    "user_hidden_states = tf.where(\n",
    "    ended_users_mask,\n",
    "    tf.zeros(tf.shape(user_hidden_states)),\n",
    "    user_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict new Session Representation\n",
    "\n",
    "new_session_hidden_states_seed, new_user_hidden_states = user_rnn.apply(tf.expand_dims(session_hidden_states, 1), initial_state=user_hidden_states)\n",
    "new_session_hidden_states = user2session_layer.apply(new_session_hidden_states_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select predicted Sessions where necessary\n",
    "\n",
    "session_hidden_states = tf.where(\n",
    "    ended_sessions_mask,\n",
    "    new_session_hidden_states,\n",
    "    session_hidden_states)\n",
    "\n",
    "user_hidden_states = tf.where(\n",
    "    ended_sessions_mask,\n",
    "    new_user_hidden_states,\n",
    "    user_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new masks\n",
    "ended_sessions_mask = tf.cast(\n",
    "    tf.where(\n",
    "        tf.equal(features['ProductId'], -1), \n",
    "        tf.ones(tf.shape(ended_sessions_mask)), \n",
    "        tf.zeros(tf.shape(ended_sessions_mask))), \n",
    "    tf.bool)\n",
    "\n",
    "ending_sessions_mask = tf.cast(\n",
    "    tf.where(\n",
    "        tf.equal(labels['ProductId'], -1), \n",
    "        tf.ones(tf.shape(ending_sessions_mask)), \n",
    "        tf.zeros(tf.shape(ending_sessions_mask))), \n",
    "    tf.bool)\n",
    "\n",
    "ended_users_mask = tf.cast(\n",
    "    tf.where(\n",
    "        tf.equal(features['UserId'], -1), \n",
    "        tf.ones(tf.shape(ended_users_mask)), \n",
    "        tf.zeros(tf.shape(ended_users_mask))), \n",
    "    tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "\n",
    "relevant_sessions_mask = tf.logical_not(tf.logical_or(ended_sessions_mask, ending_sessions_mask))\n",
    "\n",
    "relevant_input_embeddings = tf.map_fn(\n",
    "    lambda x: tf.nn.embedding_lookup(embeddings, x[0])\n",
    "    if tf.greater(x[1], tf.constant(0, dtype=tf.int64))\n",
    "    else tf.zeros(params['embedding_size']),\n",
    "    tf.stack([\n",
    "        features['EmbeddingId'],\n",
    "        tf.cast(relevant_sessions_mask, tf.int64)],\n",
    "        axis=1),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "relevant_hidden_states = tf.where(\n",
    "    relevant_sessions_mask,\n",
    "    session_hidden_states,\n",
    "    tf.zeros(tf.shape(session_hidden_states))\n",
    ")\n",
    "\n",
    "relevant_labels = tf.boolean_mask(labels['EmbeddingId'], relevant_sessions_mask)\n",
    "\n",
    "predicted_embeddings, new_session_hidden_states = session_rnn.apply(\n",
    "    tf.expand_dims(relevant_input_embeddings, 1),\n",
    "    initial_state=relevant_hidden_states)\n",
    "\n",
    "predicted_embeddings = tf.boolean_mask(\n",
    "    predicted_embeddings,\n",
    "    relevant_sessions_mask)\n",
    "\n",
    "session_hidden_states = tf.where(\n",
    "    relevant_sessions_mask,\n",
    "    new_session_hidden_states,\n",
    "    session_hidden_states)\n",
    "\n",
    "softmax_predictions = tf.matmul(predicted_embeddings, softmax_weights, transpose_b=True) + softmax_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Hitrate\n",
    "\n",
    "in_top_k = tf.nn.in_top_k(softmax_predictions, relevant_labels, 5)\n",
    "hitrate = tf.divide(\n",
    "    tf.reduce_sum(tf.cast(in_top_k, tf.int64)),\n",
    "    tf.shape(labels)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Loss Function\n",
    "\n",
    "negative_samples_weights = tf.nn.embedding_lookup(softmax_weights, relevant_labels)\n",
    "negative_samples_biases = tf.nn.embedding_lookup(softmax_biases, relevant_labels)\n",
    "\n",
    "logits = tf.matmul(predicted_embeddings, negative_samples_weights, transpose_b=True) + negative_samples_biases\n",
    "yhat = tf.nn.softmax(logits) # for each of the examples in the batch we select the remainder of the minibatch as negative examples\n",
    "\n",
    "# TOP 1 Loss function\n",
    "yhatT = tf.transpose(yhat)\n",
    "term1 = tf.reduce_mean(tf.nn.sigmoid(-tf.diag_part(yhat)+yhatT)+tf.nn.sigmoid(yhatT**2), axis=0)\n",
    "term2 = tf.nn.sigmoid(tf.diag_part(yhat)**2) / batch_size.value\n",
    "loss = tf.reduce_mean(term1 - term2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2867, shape=(), dtype=float32, numpy=0.9640579>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c9a5cbb247ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/code-JIyQgSPM/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/code-JIyQgSPM/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m       raise RuntimeError(\n\u001b[0;32m--> 481\u001b[0;31m           \u001b[0;34m\"`loss` passed to Optimizer.compute_gradients should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m           \"be a function when eager execution is enabled.\")\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=639, shape=(10, 25), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_hidden_states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_models",
   "language": "python",
   "name": "thesis_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
