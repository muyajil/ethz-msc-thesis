{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Dataset\n",
    "\n",
    "- This dataset is used to implement the baseline from the paper \"Personalizing Session based Recommendations with Hierarchical RNNs\" -> resources/papers/personalizing_session_based_rec.pdf\n",
    "- This dataset is generated from the OnlineShopTrafficTracking Table in BigQuery\n",
    "- Clean out bots using: https://github.com/monperrus/crawler-user-agents/blob/master/crawler-user-agents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "TESTMODE = False\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ResourceWarning)\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import StringIO\n",
    "from dg_ml_core.datastores import gcs_utils\n",
    "from dg_ml_core.file import get_file_handle, get_paths_with_prefix, save_to_file, file_exists, copy_file\n",
    "from dg_ml_core.collections import dict_ops\n",
    "from dg_ml_core.datastores import gcs_utils\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import pprint\n",
    "from statistics import mean, median, stdev\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_events_per_product = 5\n",
    "min_events_per_session = 3\n",
    "min_sessions_per_user = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bots_list():\n",
    "    url = 'https://raw.githubusercontent.com/monperrus/crawler-user-agents/master/crawler-user-agents.json'\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.content)\n",
    "    all_instances = [item for sublist in map(lambda x: x['instances'], data) for item in sublist]\n",
    "    return all_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Query in BQ\n",
    "\n",
    "- Here we extract the relevant features out of the large collection of visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query \n",
      "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
      "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
      "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show' AND UserId > 0\n",
      " AND _PARTITIONTIME < TIMESTAMP(\"2019-02-11\"). \n",
      "You have 5 seconds to cancel...\n",
      "Running Job baseline_dataset_query_df2b2811-b0bd-440f-9371-988a33ccd80e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7fad64ce9da0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution done\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
    "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
    "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show' AND UserId > 0\n",
    "\"\"\"\n",
    "\n",
    "if TESTMODE:\n",
    "    query += ' AND _PARTITIONTIME = TIMESTAMP(\"2019-02-11\")'\n",
    "else:\n",
    "    query += ' AND _PARTITIONTIME < TIMESTAMP(\"2019-02-11\")'\n",
    "    \n",
    "print('Executing query {}. \\nYou have 5 seconds to cancel...'.format(query))\n",
    "time.sleep(5)\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('user_visits')\n",
    "\n",
    "job_config = bigquery.job.QueryJobConfig(\n",
    "    allow_large_results=True, \n",
    "    destination=table_ref,\n",
    "    write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE)\n",
    "\n",
    "query_job = client.query(query, job_config=job_config, job_id_prefix='baseline_dataset_query_', location='EU')\n",
    "print('Running Job {}'.format(query_job.job_id))\n",
    "query_job.result()\n",
    "\n",
    "print('Query execution done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract to GCS\n",
    "\n",
    "- Extract the table containing the relevant features to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Job baseline_dataset_extract_42af8c64-687a-4897-a333-1ec49119f06c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7fad64d8c630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction done\n"
     ]
    }
   ],
   "source": [
    "destination_uri = 'gs://ma-muy/01_sessions_by_user/01_raw/*.csv'\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('user_visits')\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    location='EU',\n",
    "    job_id_prefix='user_visits_extract')\n",
    "\n",
    "print('Running Job {}'.format(extract_job.job_id))\n",
    "extract_job.result()\n",
    "print('Extraction done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "- Here we clean the data.\n",
    "- Specifically there are two steps:\n",
    "  - Clean out bot visits\n",
    "  - Merge LastLoggedInUserId and UserId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(source, target):\n",
    "    col_types = {\"ProductId\": 'Float64', \n",
    "             \"UserId\": 'Float64', \n",
    "             \"UserAgent\": str, \n",
    "             \"LastLoggedInUserId\": 'Float64', \n",
    "             \"SessionId\": 'Float64', \n",
    "             \"Timestamp\": 'Float64'}\n",
    "    \n",
    "    df = pd.read_csv(source, dtype=col_types).fillna(-1)\n",
    "    bots = get_bots_list()\n",
    "    df = df[~df.UserAgent.isin(bots)]\n",
    "    df = df.dropna(subset=[\"ProductId\"])\n",
    "    \n",
    "    no_user_id_mask = df.UserId == -1\n",
    "    df.loc[no_user_id_mask, 'UserId'] = df.loc[no_user_id_mask, 'LastLoggedInUserId']\n",
    "    \n",
    "    df.to_csv(target, index=False, columns=['UserId', 'ProductId', 'SessionId', 'Timestamp'])\n",
    "\n",
    "    return target\n",
    "\n",
    "##################################################################\n",
    "\n",
    "if TESTMODE:\n",
    "    print('Processing example.csv')\n",
    "    df = clean_dataset('example.csv', 'example_clean.csv')\n",
    "    \n",
    "else:\n",
    "    raw_data_prefix = 'gs://ma-muy/01_sessions_by_user/01_raw/'\n",
    "    cleaned_data_prefix = 'gs://ma-muy/01_sessions_by_user/02_clean/'\n",
    "    \n",
    "    client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    raw_paths = get_paths_with_prefix(raw_data_prefix)\n",
    "    print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "    for raw_path in raw_paths:\n",
    "        clean_path = cleaned_data_prefix + gcs_utils.get_file_name(raw_path)\n",
    "\n",
    "        print('=', end='', flush=True)\n",
    "        source = get_file_handle(raw_path, gcs_client=client)\n",
    "        target = StringIO()\n",
    "        \n",
    "        target = clean_dataset(source, target)\n",
    "        \n",
    "        save_to_file(clean_path, target.getvalue(), gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sessions\n",
    "\n",
    "- In this step we will merge all the single visit events into sessions\n",
    "- Further we merge all sessions to the specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def merge_sessions(reader):\n",
    "    sessions_by_user = dict()\n",
    "    for row in reader:\n",
    "        user_id = str(int(float(row['UserId'])))\n",
    "        session_id = str(int(float(row['SessionId'])))\n",
    "        product_id = int(float(row['ProductId']))\n",
    "        timestamp = int(float(row['Timestamp']))\n",
    "        \n",
    "        if user_id not in sessions_by_user:\n",
    "            sessions_by_user[user_id] = dict()\n",
    "        \n",
    "        if session_id not in sessions_by_user[user_id]:\n",
    "            sessions_by_user[user_id][session_id] = dict()\n",
    "            sessions_by_user[user_id][session_id]['Events'] = []\n",
    "        \n",
    "        sessions_by_user[user_id][session_id]['Events'].append(\n",
    "            {\n",
    "                \"ProductId\": product_id,\n",
    "                \"Timestamp\": timestamp\n",
    "            })\n",
    "        \n",
    "        first_event_ts = min(map(lambda x: int(x['Timestamp']), sessions_by_user[user_id][session_id]['Events']))\n",
    "        sessions_by_user[user_id][session_id]['StartTime'] = first_event_ts\n",
    "    return sessions_by_user, unique_products, unique_users, unique_sessions\n",
    "\n",
    "##################################################################\n",
    "\n",
    "if TESTMODE:\n",
    "    reader = csv.DictReader(open('example_clean.csv'))\n",
    "    \n",
    "    unique_products = set()\n",
    "    unique_users = set()\n",
    "    unique_sessions = set()\n",
    "    \n",
    "    sessions_by_user = merge_sessions(reader)\n",
    "    \n",
    "    dict_ops.save_dict('example_merged.json', sessions_by_user)\n",
    "\n",
    "else:\n",
    "    cleaned_data_prefix = 'gs://ma-muy/01_sessions_by_user/02_clean/'\n",
    "    merged_data_prefix = 'gs://ma-muy/01_sessions_by_user/03_merged/'\n",
    "    \n",
    "    client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    clean_paths = get_paths_with_prefix(cleaned_data_prefix)\n",
    "    print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "    for clean_path in clean_paths:\n",
    "        merged_path = (merged_data_prefix + gcs_utils.get_file_name(clean_path)).replace('csv', 'json')\n",
    "        \n",
    "        print('=', end='', flush=True)\n",
    "        source = get_file_handle(clean_path, gcs_client=client)\n",
    "        reader = csv.DictReader(source)\n",
    "        \n",
    "        sessions_by_user = merge_sessions(reader)\n",
    "        \n",
    "        dict_ops.save_dict(merged_path, sessions_by_user, gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge shards\n",
    "\n",
    "- As of now we have several shards, containing the sessions aggregated to the user level.\n",
    "- The merging of the shards is the most time consuming part of the data generation process. \n",
    "- We need to merge all sessions of a specific user into one datastructure.\n",
    "- In production we will be dealing with daily shards, which makes the generation of the dataset easier\n",
    "- However in this case we will be dealing with full exports, therefore we cannot assume that a shard is from one day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sessions_by_user(shard, merged_shards_prefix, num_target_files):\n",
    "    \n",
    "    gcs_client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    path = ''\n",
    "    new_path = ''\n",
    "    output_dict = dict()\n",
    "    \n",
    "    for i in range(num_target_files):\n",
    "        relevant_user_ids = list(filter(lambda x: int(x) % num_target_files == i, shard.keys()))\n",
    "        \n",
    "        path = merged_shards_prefix + str(i) + '.json' # Add a datestamp hierarchy\n",
    "        \n",
    "        if file_exists(path):\n",
    "            output_dict = dict_ops.load_dict(path, gcs_client=gcs_client)\n",
    "        else:\n",
    "            output_dict = dict()\n",
    "            \n",
    "        for user_id in relevant_user_ids:\n",
    "            if int(user_id) > 0:\n",
    "                for session_id in shard[user_id]:\n",
    "                    if user_id not in output_dict:\n",
    "                        output_dict[user_id] = dict()\n",
    "\n",
    "                    if session_id not in output_dict[user_id]:\n",
    "                        output_dict[user_id][session_id] = shard[user_id][session_id]\n",
    "\n",
    "                    else:\n",
    "                        merged_events = output_dict[user_id][session_id]['Events'] + shard[user_id][session_id]['Events']\n",
    "                        merged_events_str = map(lambda x: json.dumps(x), merged_events)\n",
    "                        unique_events_str = set(merged_events_str)\n",
    "                        unique_events = list(map(lambda x: json.loads(x), unique_events_str))\n",
    "                        output_dict[user_id][session_id]['Events'] = unique_events\n",
    "                        output_dict[user_id][session_id]['StartTime'] = min(map(lambda x: int(x['Timestamp']), unique_events))\n",
    "\n",
    "        dict_ops.save_dict(path, output_dict, gcs_client=gcs_client)\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "NUM_TARGET_FILES = 100\n",
    "if TESTMODE:\n",
    "    shard = json.load(open('example_merged.json'))\n",
    "    generate_sessions_by_user(shard, 'sessions_by_user/', NUM_TARGET_FILES)\n",
    "else:\n",
    "    merged_data_prefix = 'gs://ma-muy/01_sessions_by_user/02_merged/'\n",
    "    sessions_by_user_prefix = 'gs://ma-muy/01_sessions_by_user/04_sessions_by_user/'\n",
    "    temp_sessions_by_user_prefix = 'temp_sessions_by_user/'\n",
    "    \n",
    "    client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    merged_paths = get_paths_with_prefix(merged_data_prefix)\n",
    "    print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "    for merged_path in merged_paths[-2:]:\n",
    "        \n",
    "        print('=', end='', flush=True)\n",
    "        source = dict_ops.load_dict(merged_path)\n",
    "        \n",
    "        generate_sessions_by_user(source, temp_sessions_by_user_prefix, NUM_TARGET_FILES)\n",
    "    \n",
    "    temp_files = get_paths_with_prefix(temp_sessions_by_user_prefix)\n",
    "    \n",
    "    print('Uploading Files')\n",
    "    \n",
    "    for temp_file in temp_files:\n",
    "        print('=', end='')\n",
    "        file_name = temp_file.rsplit('/', 1)[1]\n",
    "        if 'ipynb' not in file_name:\n",
    "            target_uri = sessions_by_user_prefix + file_name\n",
    "            copy_file(temp_file, target_uri, gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data & Collect Statistics\n",
    "\n",
    "- At this point we have the data in a form that is nice for preprocessing.\n",
    "- During preprocessing we can also collect dataset stats\n",
    "- According to the paper there are several steps to preprocess the session data:\n",
    "    - Remove Items with low support (which threshold to use? 10 vs. 20)\n",
    "    - Remove sessions with less than 3 items\n",
    "    - Remove users with less than 5 sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing                                                                                        |\n",
      "===================================================================================================="
     ]
    }
   ],
   "source": [
    "def filter_sessions_and_users(input_dict, \n",
    "                              events_per_products):\n",
    "    \n",
    "    output_dict = dict()\n",
    "    \n",
    "    for user_id in input_dict:\n",
    "        output_dict[user_id] = dict()\n",
    "        for session_id in input_dict[user_id]:\n",
    "            if len(input_dict[user_id][session_id]['Events']) >= min_events_per_session:\n",
    "                output_dict[user_id][session_id] = input_dict[user_id][session_id]\n",
    "        if len(output_dict[user_id]) < min_sessions_per_user:\n",
    "            _ = output_dict.pop(user_id, None)\n",
    "        else:\n",
    "            for session_id in output_dict[user_id]:\n",
    "                product_ids = list(map(lambda x: x['ProductId'], output_dict[user_id][session_id]['Events']))\n",
    "                for product_id in product_ids:\n",
    "                    if product_id in events_per_products:\n",
    "                        events_per_products[product_id] += 1\n",
    "                    else:\n",
    "                        events_per_products[product_id] = 1\n",
    "    \n",
    "    return output_dict, events_per_products\n",
    "\n",
    "##################################################################\n",
    "\n",
    "events_per_product = dict()\n",
    "\n",
    "sessions_by_user_prefix = 'gs://ma-muy/sessions_by_user/01_sessions_by_user/'\n",
    "filtered_sessions_and_users_prefix = 'gs://ma-muy/02_preprocessed/01_filtered_sessions_and_users/'\n",
    "    \n",
    "client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "paths = get_paths_with_prefix(sessions_by_user_prefix)\n",
    "\n",
    "print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "for path in paths:\n",
    "    \n",
    "    print('=', end='', flush=True)\n",
    "    input_dict = dict_ops.load_dict(path, gcs_client=client)\n",
    "    \n",
    "    output_dict, events_per_product = filter_sessions_and_users(input_dict,\n",
    "                                                            events_per_product)\n",
    "    \n",
    "    file_name = gcs_utils.get_file_name(path)\n",
    "    output_path = filtered_sessions_and_users_prefix + file_name\n",
    "    dict_ops.save_dict(output_path, output_dict, gcs_client=client)\n",
    "    \n",
    "dict_ops.save_dict('gs://ma-muy/02_preprocessed/02_events_per_product.json', events_per_product, gcs_client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Products: 1126156\n",
      "Products with low support: 583796\n",
      "Products with enough support: 542360\n",
      "Processing                                                                                         |\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def filter_products(input_dict, products_to_filter, events_per_product):\n",
    "    \n",
    "    output_dict = dict()\n",
    "    num_filtered = 0\n",
    "    \n",
    "    for user_id in input_dict:\n",
    "        output_dict[user_id] = dict()\n",
    "        for session_id in input_dict[user_id]:\n",
    "\n",
    "            filtered_events = list(filter(lambda x: x['ProductId'] not in products_to_filter, input_dict[user_id][session_id]['Events']))\n",
    "\n",
    "            if len(filtered_events) >= min_events_per_session:\n",
    "                output_dict[user_id][session_id] = dict()\n",
    "                output_dict[user_id][session_id]['Events'] = filtered_events\n",
    "                output_dict[user_id][session_id]['StartTime'] = min(map(lambda x: int(x['Timestamp']), output_dict[user_id][session_id]['Events']))\n",
    "            else:\n",
    "                # keep events_per_products accurate -> if we do not add a session we remove the counts generated by that\n",
    "                for product_id in map(lambda x: x['ProductId'], input_dict[user_id][session_id]['Events']):\n",
    "                    if str(product_id) in events_per_product:\n",
    "                        events_per_product[str(product_id)] -= 1\n",
    "                \n",
    "    return output_dict, events_per_product\n",
    "\n",
    "##################################################################\n",
    "\n",
    "client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "filtered_users_and_sessions_prefix = 'gs://ma-muy/02_preprocessed/01_filtered_sessions_and_users/'\n",
    "filtered_products_prefix = 'gs://ma-muy/02_preprocessed/03_filtered_products/'\n",
    "events_per_product = dict_ops.load_dict('gs://ma-muy/02_preprocessed/02_events_per_product.json', gcs_client=client)\n",
    "products_to_filter = set(list(map(lambda x: int(x), filter(lambda x: events_per_product[x] < min_events_per_product, events_per_product))))\n",
    "\n",
    "print(\"Total Products:\", len(events_per_product.keys()))\n",
    "print(\"Products with low support:\", len(products_to_filter))\n",
    "      \n",
    "for product_id in products_to_filter:\n",
    "    _ = events_per_product.pop(str(product_id), None)\n",
    "    \n",
    "print(\"Products with enough support:\", len(events_per_product.keys()))\n",
    "\n",
    "paths = get_paths_with_prefix(filtered_users_and_sessions_prefix)\n",
    "print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "for path in paths:\n",
    "    print('=', end='', flush=True)\n",
    "    input_dict = dict_ops.load_dict(path, gcs_client=client)\n",
    "    \n",
    "    output_dict, events_per_product = filter_products(\n",
    "        input_dict, \n",
    "        products_to_filter, \n",
    "        events_per_product)\n",
    "    \n",
    "    file_name = gcs_utils.get_file_name(path)\n",
    "    output_path = filtered_products_prefix + file_name\n",
    "    dict_ops.save_dict(output_path, output_dict, gcs_client=client)\n",
    "print('')\n",
    "\n",
    "# Remove products with no events\n",
    "products_with_no_events = list(filter(lambda x: events_per_product[x] <= 0, events_per_product))\n",
    "for product_id in products_with_no_events:\n",
    "    if events_per_product[product_id] <= 0:\n",
    "        _ = events_per_product.pop(product_id, None)\n",
    "\n",
    "dict_ops.save_dict('gs://ma-muy/02_preprocessed/04_events_per_product_filtered.json', events_per_product, gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample Dataset & Collect Stats\n",
    "\n",
    "- To first enable fast iterations and second work on a comparable dataset like the one in the paper we subsample the dataset here.\n",
    "- There are two versions, one really small dataset that enables fast iterations and one intermediate dataset that compares to the ones from the papers\n",
    "- To get enough events we just take the most visited products\n",
    "- During this we can also collect the dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing                                                                                         |\n",
      "==========\n",
      "{\n",
      "  \"num_users\": 1089,\n",
      "  \"num_products\": 596,\n",
      "  \"num_sessions\": 9177,\n",
      "  \"median_sessions_per_user\": 7,\n",
      "  \"mean_sessions_per_user\": 8.426997245179063,\n",
      "  \"std_sessions_per_user\": 5.2896303837319065,\n",
      "  \"median_events_per_product\": 16.0,\n",
      "  \"mean_events_per_product\": 66.73825503355705,\n",
      "  \"std_events_per_product\": 137.64815192137576,\n",
      "  \"num_events\": 39776,\n",
      "  \"median_events_per_session\": 4,\n",
      "  \"mean_events_per_session\": 4.334314045984526,\n",
      "  \"std_events_per_session\": 2.4269700404982273\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def subsample(input_dict, \n",
    "              products_to_keep,\n",
    "              dataset_stats):\n",
    "    \n",
    "    output_dict = dict()\n",
    "    for user_id in input_dict:\n",
    "        user_dict = dict()\n",
    "        for session_id in input_dict[user_id]:\n",
    "            events = []\n",
    "            for event in input_dict[user_id][session_id]['Events']:\n",
    "                if str(event['ProductId']) in products_to_keep:\n",
    "                    events.append(event)\n",
    "            if len(events) >= min_events_per_session:\n",
    "                user_dict[session_id] = dict()\n",
    "                user_dict[session_id]['Events'] = events\n",
    "                user_dict[session_id]['StartTime'] = min(map(lambda x: int(x['Timestamp']), events))\n",
    "\n",
    "        if len(user_dict) >= min_sessions_per_user:\n",
    "            output_dict[user_id] = user_dict\n",
    "            dataset_stats['sessions_per_user'].append(len(user_dict))\n",
    "            dataset_stats['events_per_session'].extend(map(lambda x: len(user_dict[x]['Events']), user_dict))\n",
    "            dataset_stats['num_users'] += 1\n",
    "            \n",
    "    return output_dict, dataset_stats\n",
    "\n",
    "##################################################################\n",
    "\n",
    "events_per_product = dict_ops.load_dict('gs://ma-muy/02_preprocessed/04_events_per_product_filtered.json', gcs_client=client)\n",
    "\n",
    "max_products = 1000\n",
    "approx_max_users = 1000\n",
    "dataset_name = 'mini_dataset'\n",
    "dataset_stats = dict()\n",
    "dataset_stats['num_users'] = 0\n",
    "dataset_stats['num_products'] = max_products\n",
    "dataset_stats['sessions_per_user'] = []\n",
    "dataset_stats['events_per_session'] = []\n",
    "\n",
    "total_events = sum(events_per_product.values())\n",
    "probs = [x / total_events for x in events_per_product.values()]\n",
    "# products_to_keep = sorted(events_per_product.keys(), key=lambda x: events_per_product[x], reverse=True)[:max_products]\n",
    "products_to_keep = set(np.random.choice(list(events_per_product.keys()), max_products, p=probs, replace=False))\n",
    "\n",
    "preprocessed_prefix = 'gs://ma-muy/02_preprocessed/03_filtered_products/'\n",
    "dataset_prefix = 'gs://ma-muy/03_datasets/' + dataset_name + '/01_complete/'\n",
    "\n",
    "events_per_product_temp = dict()\n",
    "\n",
    "paths = get_paths_with_prefix(preprocessed_prefix)\n",
    "np.random.shuffle(paths)\n",
    "print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "for path in paths:\n",
    "    print('=', end='', flush=True)\n",
    "    if dataset_stats['num_users'] > max_users:\n",
    "        break\n",
    "    \n",
    "    input_dict = dict_ops.load_dict(path, gcs_client=client)\n",
    "    \n",
    "    output_dict, dataset_stats = subsample(input_dict, products_to_keep, dataset_stats)\n",
    "    \n",
    "    if max_products != len(events_per_product.keys()):\n",
    "        for user_id in output_dict:\n",
    "            for session_id in output_dict[user_id]:\n",
    "                for event in output_dict[user_id][session_id]['Events']:\n",
    "                    if str(event['ProductId']) in events_per_product_temp:\n",
    "                        events_per_product_temp[str(event['ProductId'])] += 1\n",
    "                    else:\n",
    "                        events_per_product_temp[str(event['ProductId'])] = 1\n",
    "        \n",
    "    if output_dict:\n",
    "        file_name = gcs_utils.get_file_name(path)\n",
    "        output_path = dataset_prefix + file_name\n",
    "        dict_ops.save_dict(output_path, output_dict, gcs_client=client)\n",
    "print('')\n",
    "\n",
    "if max_products != len(events_per_product.keys()):\n",
    "    events_per_product = events_per_product_temp\n",
    "\n",
    "dataset_stats['events_per_product'] = events_per_product\n",
    "dict_ops.save_dict(dataset_prefix[:-12] + '02_events_per_product.json', events_per_product, gcs_client=client)\n",
    "\n",
    "dataset_stats['num_sessions'] = sum(dataset_stats['sessions_per_user'])\n",
    "dataset_stats['median_sessions_per_user'] = median(dataset_stats['sessions_per_user'])\n",
    "dataset_stats['mean_sessions_per_user'] = mean(dataset_stats['sessions_per_user'])\n",
    "dataset_stats['std_sessions_per_user'] = stdev(dataset_stats['sessions_per_user'])\n",
    "del dataset_stats['sessions_per_user']\n",
    "\n",
    "dataset_stats['median_events_per_product'] = median(dataset_stats['events_per_product'].values())\n",
    "dataset_stats['mean_events_per_product'] = mean(dataset_stats['events_per_product'].values())\n",
    "dataset_stats['std_events_per_product'] = stdev(dataset_stats['events_per_product'].values())\n",
    "dataset_stats['num_products'] = len(dataset_stats['events_per_product'].keys())\n",
    "del dataset_stats['events_per_product']\n",
    "\n",
    "dataset_stats['num_events'] = sum(dataset_stats['events_per_session'])\n",
    "dataset_stats['median_events_per_session'] = median(dataset_stats['events_per_session'])\n",
    "dataset_stats['mean_events_per_session'] = mean(dataset_stats['events_per_session'])\n",
    "dataset_stats['std_events_per_session'] = stdev(dataset_stats['events_per_session'])\n",
    "del dataset_stats['events_per_session']\n",
    "\n",
    "print(json.dumps(dataset_stats, indent=2))\n",
    "dict_ops.save_dict(dataset_prefix[:-12] + '03_dataset_stats.json', dataset_stats, gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embedding Dictionary\n",
    "\n",
    "- Since we will be using a one-hot encoding or later an embedding for the products we need to map those to a contiguous Id space\n",
    "- Therefore we need to go through all the products and generate an embedding dictionary, and add this feature to the features generated in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing|\n",
      "=========\n",
      "{\n",
      "  \"User\": 1089,\n",
      "  \"Product\": 596\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def generate_embedding_ids(input_dict, embedding_dict, next_embedding_id):\n",
    "    output_dict = dict()\n",
    "    for user_id in input_dict:\n",
    "        \n",
    "        if user_id not in embedding_dict['User']['ToEmbedding']:\n",
    "            embedding_dict['User']['ToEmbedding'][user_id] = next_embedding_id['User']\n",
    "            embedding_dict['User']['FromEmbedding'][str(next_embedding_id['User'])] = int(user_id)\n",
    "            next_embedding_id['User'] += 1\n",
    "        \n",
    "        output_dict[user_id] = dict()\n",
    "        for session_id in input_dict[user_id]:\n",
    "            \n",
    "            output_dict[user_id][session_id] = dict()\n",
    "            output_dict[user_id][session_id]['StartTime'] = input_dict[user_id][session_id]['StartTime']\n",
    "            output_dict[user_id][session_id]['Events'] = []\n",
    "            for event in input_dict[user_id][session_id]['Events']:\n",
    "                \n",
    "                if str(event['ProductId']) in embedding_dict['Product']['ToEmbedding']:\n",
    "                    event['EmbeddingId'] = embedding_dict['Product']['ToEmbedding'][str(event['ProductId'])]\n",
    "                else:\n",
    "                    embedding_dict['Product']['ToEmbedding'][str(event['ProductId'])] = next_embedding_id['Product']\n",
    "                    embedding_dict['Product']['FromEmbedding'][str(next_embedding_id)] = event['ProductId']\n",
    "                    next_embedding_id['Product'] += 1\n",
    "                    event['EmbeddingId'] = embedding_dict['Product']['ToEmbedding'][str(event['ProductId'])]\n",
    "                event['UserEmbeddingId'] = embedding_dict['User']['ToEmbedding'][user_id]\n",
    "                output_dict[user_id][session_id]['Events'].append(event)\n",
    "    return output_dict, embedding_dict, next_embedding_id\n",
    "\n",
    "##################################################################\n",
    "\n",
    "dataset_name = 'mini_dataset'\n",
    "dataset_prefix = 'gs://ma-muy/03_datasets/' + dataset_name + '/01_complete/'\n",
    "embedded_prefix = 'gs://ma-muy/03_datasets/' + dataset_name + '/04_embedded/'\n",
    "\n",
    "client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "embedding_dict = dict()\n",
    "embedding_dict['User'] = dict()\n",
    "embedding_dict['User']['ToEmbedding'] = dict()\n",
    "embedding_dict['User']['FromEmbedding'] = dict()\n",
    "embedding_dict['Product'] = dict()\n",
    "embedding_dict['Product']['ToEmbedding'] = dict()\n",
    "embedding_dict['Product']['FromEmbedding'] = dict()\n",
    "\n",
    "next_embedding_id = dict()\n",
    "next_embedding_id['User'] = 0\n",
    "next_embedding_id['Product'] = 0\n",
    "\n",
    "paths = get_paths_with_prefix(dataset_prefix)\n",
    "print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "for path in paths:\n",
    "\n",
    "    print('=', end='', flush=True)\n",
    "    file_name = gcs_utils.get_file_name(path)\n",
    "\n",
    "    input_dict = dict_ops.load_dict(path, gcs_client=client)\n",
    "\n",
    "    output_dict, embedding_dict, next_embedding_id = generate_embedding_ids(input_dict, embedding_dict, next_embedding_id)\n",
    "\n",
    "    dict_ops.save_dict(embedded_prefix + file_name, output_dict, gcs_client=client)\n",
    "print('')\n",
    "print(json.dumps(next_embedding_id, indent=2))\n",
    "dict_ops.save_dict(dataset_prefix[:-12] + '05_embedding_dict.json', embedding_dict, gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train, Validation and Test Dataset\n",
    "\n",
    "- For each user keep the last session in the test set\n",
    "- The rest represents the training set\n",
    "- The last session of users in the training set is extracted again and used as the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing|\n",
      "========="
     ]
    }
   ],
   "source": [
    "def split_training_set(input_dict):\n",
    "    train_set = dict()\n",
    "    eval_set = dict()\n",
    "    test_set = dict()\n",
    "    for user_id in input_dict:\n",
    "        train_set[user_id] = dict()\n",
    "        eval_set[user_id] = dict()\n",
    "        test_set[user_id] = dict()\n",
    "        \n",
    "        sorted_sessions = sorted(map(lambda x: (x, input_dict[user_id][x]), input_dict[user_id].keys()), key=lambda y: y[1]['StartTime'])\n",
    "        for idx, sorted_session in enumerate(sorted_sessions):\n",
    "            if idx == len(sorted_sessions) - 1:\n",
    "                test_set[user_id][sorted_session[0]] = sorted_session[1]\n",
    "            elif idx == len(sorted_session) - 2:\n",
    "                eval_set[user_id][sorted_session[0]] = sorted_session[1]\n",
    "            else:\n",
    "                train_set[user_id][sorted_session[0]] = sorted_session[1]\n",
    "    \n",
    "    return train_set, eval_set, test_set\n",
    "\n",
    "##################################################################\n",
    "\n",
    "dataset_name = 'mini_dataset'\n",
    "embedded_prefix = 'gs://ma-muy/03_datasets/' + dataset_name + '/04_embedded/'\n",
    "train_prefix = 'gs://ma-muy/03_datasets/' + dataset_name + '/05_train/'\n",
    "eval_prefix = 'gs://ma-muy/03_datasets/' + dataset_name + '/06_eval/'\n",
    "test_prefix = 'gs://ma-muy/03_datasets/' + dataset_name + '/07_test/'\n",
    "\n",
    "client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "paths = get_paths_with_prefix(embedded_prefix)\n",
    "print('Processing' + ' '*(len(paths) - 11) + '|')\n",
    "for path in paths:\n",
    "    print('=', end='', flush=True)\n",
    "    file_name = gcs_utils.get_file_name(path)\n",
    "    \n",
    "    input_dict = dict_ops.load_dict(path, gcs_client=client)\n",
    "    \n",
    "    train_set, eval_set, test_set = split_training_set(input_dict)\n",
    "    \n",
    "    dict_ops.save_dict(train_prefix + file_name, train_set, gcs_client=client)\n",
    "    dict_ops.save_dict(eval_prefix + file_name, eval_set, gcs_client=client)\n",
    "    dict_ops.save_dict(test_prefix + file_name, test_set, gcs_client=client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
