{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Dataset\n",
    "\n",
    "- This dataset is used to implement the baseline from the paper \"Personalizing Session based Recommendations with Hierarchical RNNs\" -> resources/papers/personalizing_session_based_rec.pdf\n",
    "- This dataset is generated from the OnlineShopTrafficTracking Table in BigQuery\n",
    "- Clean out bots using: https://github.com/monperrus/crawler-user-agents/blob/master/crawler-user-agents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "TESTMODE = False\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ResourceWarning)\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import StringIO\n",
    "from dg_ml_core.datastores import gcs_utils\n",
    "from dg_ml_core.file import get_file_handle, get_paths_with_prefix, save_to_file, file_exists, copy_file\n",
    "from dg_ml_core.collections import dict_ops\n",
    "from dg_ml_core.datastores import gcs_utils\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bots_list():\n",
    "    url = 'https://raw.githubusercontent.com/monperrus/crawler-user-agents/master/crawler-user-agents.json'\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.content)\n",
    "    all_instances = [item for sublist in map(lambda x: x['instances'], data) for item in sublist]\n",
    "    return all_instances\n",
    "\n",
    "def clean_dataset(source, target):\n",
    "    col_types = {\"ProductId\": 'Float64', \n",
    "             \"UserId\": 'Float64', \n",
    "             \"UserAgent\": str, \n",
    "             \"LastLoggedInUserId\": 'Float64', \n",
    "             \"SessionId\": 'Float64', \n",
    "             \"Timestamp\": 'Float64'}\n",
    "    \n",
    "    df = pd.read_csv(source, dtype=col_types).fillna(-1)\n",
    "    bots = get_bots_list()\n",
    "    df = df[~df.UserAgent.isin(bots)]\n",
    "    df = df.dropna(subset=[\"ProductId\"])\n",
    "    \n",
    "    no_user_id_mask = df.UserId == -1\n",
    "    df.loc[no_user_id_mask, 'UserId'] = df.loc[no_user_id_mask, 'LastLoggedInUserId']\n",
    "    \n",
    "    df.to_csv(target, index=False, columns=['UserId', 'ProductId', 'SessionId', 'Timestamp'])\n",
    "\n",
    "    return target\n",
    "\n",
    "def merge_sessions(reader, unique_products, unique_users, unique_sessions):\n",
    "    sessions_by_user = dict()\n",
    "    for row in reader:\n",
    "        user_id = str(int(float(row['UserId'])))\n",
    "        session_id = str(int(float(row['SessionId'])))\n",
    "        product_id = int(float(row['ProductId']))\n",
    "        timestamp = int(float(row['Timestamp']))\n",
    "        \n",
    "        if user_id not in sessions_by_user:\n",
    "            sessions_by_user[user_id] = dict()\n",
    "        \n",
    "        if session_id not in sessions_by_user[user_id]:\n",
    "            sessions_by_user[user_id][session_id] = dict()\n",
    "            sessions_by_user[user_id][session_id]['Events'] = []\n",
    "        \n",
    "        sessions_by_user[user_id][session_id]['Events'].append(\n",
    "            {\n",
    "                \"ProductId\": product_id,\n",
    "                \"Timestamp\": timestamp\n",
    "            })\n",
    "        \n",
    "        unique_products.add(product_id)\n",
    "        unique_users.add(int(user_id))\n",
    "        unique_sessions.add(int(session_id))\n",
    "        \n",
    "        first_event_ts = min(map(lambda x: int(x['Timestamp']), sessions_by_user[user_id][session_id]['Events']))\n",
    "        sessions_by_user[user_id][session_id]['StartTime'] = first_event_ts\n",
    "    return sessions_by_user, unique_products, unique_users, unique_sessions\n",
    "\n",
    "def generate_sessions_by_user(shard, merged_shards_prefix, num_target_files):\n",
    "    \n",
    "    gcs_client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    path = ''\n",
    "    new_path = ''\n",
    "    output_dict = dict()\n",
    "    \n",
    "    for i in range(num_target_files):\n",
    "#         print('Processing users with remainder {}'.format(i))\n",
    "        print('=', end='', flush=True)\n",
    "        relevant_user_ids = list(filter(lambda x: int(x) % num_target_files == i, shard.keys()))\n",
    "        \n",
    "        path = merged_shards_prefix + str(i) + '.json' # Add a datestamp hierarchy\n",
    "        \n",
    "        if file_exists(path):\n",
    "            output_dict = dict_ops.load_dict(path, gcs_client=gcs_client)\n",
    "        else:\n",
    "            output_dict = dict()\n",
    "            \n",
    "        for user_id in relevant_user_ids:\n",
    "            if int(user_id) > 0:\n",
    "                # TODO: Collect session stats here\n",
    "                for session_id in shard[user_id]:\n",
    "                    if user_id not in output_dict:\n",
    "                        output_dict[user_id] = dict()\n",
    "\n",
    "                    if session_id not in output_dict[user_id]:\n",
    "                        output_dict[user_id][session_id] = shard[user_id][session_id]\n",
    "\n",
    "                    else:\n",
    "                        merged_events = output_dict[user_id][session_id]['Events'] + shard[user_id][session_id]['Events']\n",
    "                        merged_events_str = map(lambda x: json.dumps(x), merged_events)\n",
    "                        unique_events_str = set(merged_events_str)\n",
    "                        unique_events = list(map(lambda x: json.loads(x), unique_events_str))\n",
    "                        output_dict[user_id][session_id]['Events'] = unique_events\n",
    "                        output_dict[user_id][session_id]['StartTime'] = min(map(lambda x: int(x['Timestamp']), unique_events))\n",
    "\n",
    "        dict_ops.save_dict(path, output_dict, gcs_client=gcs_client)\n",
    "    print(' Finished!')\n",
    "\n",
    "def user_iterator(sessions_by_user_prefix):\n",
    "    paths = get_paths_with_prefix(sessions_by_user_prefix)\n",
    "    for path in paths:\n",
    "        merged_shard = dict_ops.load_dict(path)\n",
    "        user_ids = list(merged_shard.keys())\n",
    "        random.shuffle(user_ids)\n",
    "        for user_id in user_ids:\n",
    "            yield user_id, merged_shard[user_id]\n",
    "\n",
    "def event_iterator(user_sessions, min_events_per_session):\n",
    "    sorted_sessions = sorted(map(lambda x: user_sessions[x], user_sessions.keys()), key=lambda y: y['StartTime'])\n",
    "    for sorted_session in sorted_sessions:\n",
    "        if len(sorted_session['Events']) < min_events_per_session:\n",
    "            continue\n",
    "            \n",
    "        sorted_events = sorted(sorted_session['Events'], key=lambda z: z['Timestamp'])\n",
    "        for event in sorted_events:\n",
    "            yield event['ProductId']\n",
    "        \n",
    "        yield '<EOS>'\n",
    "        \n",
    "def get_next_event_or_none(active_user):\n",
    "    try:\n",
    "        return next(active_user['Events'])\n",
    "    except StopIteration:\n",
    "        return None\n",
    "    \n",
    "def get_next_user_or_none(users, min_events_per_session):\n",
    "    try:\n",
    "        user_id, user_sessions = next(users)\n",
    "        return {\n",
    "            'UserId': int(user_id),\n",
    "            'Events': event_iterator(user_sessions, min_events_per_session)\n",
    "        }\n",
    "    except StopIteration:\n",
    "        return None\n",
    "        \n",
    "def user_parallel_batch_iterator(batch_size, sessions_by_user_prefix, min_events_per_session):\n",
    "\n",
    "        active_users = dict()\n",
    "        users = user_iterator(sessions_by_user_prefix)\n",
    "    \n",
    "        data = [[]]*batch_size\n",
    "\n",
    "        # Initial fill of users\n",
    "        for i in range(batch_size):\n",
    "            active_users[i] = get_next_user_or_none(users, min_events_per_session)\n",
    "        \n",
    "        while True:\n",
    "            next_batch = dict()\n",
    "            for idx in active_users:\n",
    "                if active_users[idx] is None:\n",
    "                    next_batch[idx] = ('<EOU>', '<EOS>')\n",
    "                    continue\n",
    "                next_event = get_next_event_or_none(active_users[idx])\n",
    "                while next_event is None:\n",
    "                    next_user = get_next_user_or_none(users, min_events_per_session)\n",
    "                    if next_user is None:\n",
    "                        print('There are no more new users')\n",
    "                        active_users[idx] = None\n",
    "                        break\n",
    "                    else:\n",
    "                        active_users[idx] = next_user\n",
    "                        next_event = get_next_event_or_none(active_users[idx])\n",
    "                else:\n",
    "                    next_batch[idx] = (active_users[idx]['UserId'], next_event)\n",
    "            if len(set(next_batch.values())) == 1:\n",
    "                return\n",
    "            yield list(next_batch.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Query in BQ\n",
    "\n",
    "- Here we extract the relevant features out of the large collection of visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query \n",
      "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
      "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
      "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show' AND UserId > 0\n",
      " AND _PARTITIONTIME < TIMESTAMP(\"2019-02-11\"). \n",
      "You have 5 seconds to cancel...\n",
      "Running Job baseline_dataset_query_df2b2811-b0bd-440f-9371-988a33ccd80e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7fad64ce9da0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution done\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
    "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
    "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show' AND UserId > 0\n",
    "\"\"\"\n",
    "\n",
    "if TESTMODE:\n",
    "    query += ' AND _PARTITIONTIME = TIMESTAMP(\"2019-02-11\")'\n",
    "else:\n",
    "    query += ' AND _PARTITIONTIME < TIMESTAMP(\"2019-02-11\")'\n",
    "    \n",
    "print('Executing query {}. \\nYou have 5 seconds to cancel...'.format(query))\n",
    "time.sleep(5)\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('baseline_dataset')\n",
    "\n",
    "job_config = bigquery.job.QueryJobConfig(\n",
    "    allow_large_results=True, \n",
    "    destination=table_ref,\n",
    "    write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE)\n",
    "\n",
    "query_job = client.query(query, job_config=job_config, job_id_prefix='baseline_dataset_query_', location='EU')\n",
    "print('Running Job {}'.format(query_job.job_id))\n",
    "query_job.result()\n",
    "\n",
    "print('Query execution done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract to GCS\n",
    "\n",
    "- Extract the table containing the relevant features to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Job baseline_dataset_extract_42af8c64-687a-4897-a333-1ec49119f06c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7fad64d8c630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction done\n"
     ]
    }
   ],
   "source": [
    "destination_uri = 'gs://ma-muy/baseline_dataset/raw/*.csv'\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('baseline_dataset')\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    location='EU',\n",
    "    job_id_prefix='baseline_dataset_extract_')\n",
    "\n",
    "print('Running Job {}'.format(extract_job.job_id))\n",
    "extract_job.result()\n",
    "print('Extraction done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "- Here we clean the data.\n",
    "- Specifically there are two steps:\n",
    "  - Clean out bot visits\n",
    "  - Merge LastLoggedInUserId and UserId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000007.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000007.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000007.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000008.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000008.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000008.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000009.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000009.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000009.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000010.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000010.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000010.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000011.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000011.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000011.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000012.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000012.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000012.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000013.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000013.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000013.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000014.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000014.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000014.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000015.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000015.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000015.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000016.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000016.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000016.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000017.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000017.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000017.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000018.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000018.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000018.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000019.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000019.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000019.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000020.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000020.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000020.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000021.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000021.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000021.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000022.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000022.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000022.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000023.csv\n",
      "Processing gs://ma-muy/baseline_dataset/raw/000000000023.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000023.csv\n"
     ]
    }
   ],
   "source": [
    "if TESTMODE:\n",
    "    print('Processing example.csv')\n",
    "    df = clean_dataset('example.csv', 'example_clean.csv')\n",
    "    \n",
    "else:\n",
    "    raw_data_prefix = 'gs://ma-muy/baseline_dataset/raw/'\n",
    "    cleaned_data_prefix = 'gs://ma-muy/baseline_dataset/clean/'\n",
    "    \n",
    "    client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    raw_paths = get_paths_with_prefix(raw_data_prefix)\n",
    "    \n",
    "    for raw_path in raw_paths:\n",
    "        clean_path = cleaned_data_prefix + gcs_utils.get_file_name(raw_path)\n",
    "\n",
    "        print('Downloading {}'.format(raw_path))\n",
    "        source = get_file_handle(raw_path, gcs_client=client)\n",
    "        target = StringIO()\n",
    "        \n",
    "        print('Processing {}'.format(raw_path))\n",
    "        target = clean_dataset(source, target)\n",
    "        \n",
    "        print('Uploading {}'.format(clean_path))\n",
    "        save_to_file(clean_path, target.getvalue(), gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sessions\n",
    "\n",
    "- In this step we will merge all the single visit events into sessions\n",
    "- Further we merge all sessions to the specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000000.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000000.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000000.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000001.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000001.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000001.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000002.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000002.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000002.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000003.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000003.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000003.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000004.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000004.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000004.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000005.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000005.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000005.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000006.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000006.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000006.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000007.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000007.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000007.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000008.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000008.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000008.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000009.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000009.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000009.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000010.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000010.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000010.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000011.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000011.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000011.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000012.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000012.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000012.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000013.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000013.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000013.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000014.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000014.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000014.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000015.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000015.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000015.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000016.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000016.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000016.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000017.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000017.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000017.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000018.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000018.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000018.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000019.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000019.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000019.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000020.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000020.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000020.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000021.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000021.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000021.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000022.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000022.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000022.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000023.csv\n",
      "Processing gs://ma-muy/baseline_dataset/clean/000000000023.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000023.json\n",
      "Number of unique Products in dataset: 1258092\n",
      "Number of unique Users in dataset: 1211522\n",
      "Number of unique Sessions in dataset: 18479343\n"
     ]
    }
   ],
   "source": [
    "if TESTMODE:\n",
    "    reader = csv.DictReader(open('example_clean.csv'))\n",
    "    \n",
    "    unique_products = set()\n",
    "    unique_users = set()\n",
    "    unique_sessions = set()\n",
    "    \n",
    "    sessions_by_user, unique_products, unique_users, unique_sessions = merge_sessions(reader, unique_products, unique_users, unique_sessions)\n",
    "    \n",
    "    dict_ops.save_dict('example_merged.json', sessions_by_user)\n",
    "\n",
    "else:\n",
    "    cleaned_data_prefix = 'gs://ma-muy/baseline_dataset/clean/'\n",
    "    merged_data_prefix = 'gs://ma-muy/baseline_dataset/merged/'\n",
    "    unique_products = set()\n",
    "    unique_users = set()\n",
    "    unique_sessions = set()\n",
    "    \n",
    "    client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    clean_paths = get_paths_with_prefix(cleaned_data_prefix)\n",
    "    for clean_path in clean_paths:\n",
    "        merged_path = (merged_data_prefix + gcs_utils.get_file_name(clean_path)).replace('csv', 'json')\n",
    "        \n",
    "        print('Downloading {}'.format(clean_path))\n",
    "        source = get_file_handle(clean_path, gcs_client=client)\n",
    "        reader = csv.DictReader(source)\n",
    "        \n",
    "        print('Processing {}'.format(clean_path))\n",
    "        sessions_by_user, unique_products, unique_users, unique_sessions = merge_sessions(reader, unique_products, unique_users, unique_sessions)\n",
    "        \n",
    "        print('Uploading {}'.format(merged_path))\n",
    "        dict_ops.save_dict(merged_path, sessions_by_user, gcs_client=client)\n",
    "\n",
    "print('Number of unique Products in dataset:', len(unique_products))\n",
    "print('Number of unique Users in dataset:', len(unique_users))\n",
    "print('Number of unique Sessions in dataset:', len(unique_sessions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge shards\n",
    "\n",
    "- As of now we have several shards, containing the sessions aggregated to the user level.\n",
    "- The merging of the shards is the most time consuming part of the data generation process. \n",
    "- We need to merge all sessions of a specific user into one datastructure.\n",
    "- In production we will be dealing with daily shards, which makes the generation of the dataset easier\n",
    "- However in this case we will be dealing with full exports, therefore we cannot assume that a shard is from one day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://ma-muy/baseline_dataset/merged/000000000022.json\n",
      "Processing gs://ma-muy/baseline_dataset/merged/000000000022.json\n",
      "==================================================================================================== Finished!\n",
      "Downloading gs://ma-muy/baseline_dataset/merged/000000000023.json\n",
      "Processing gs://ma-muy/baseline_dataset/merged/000000000023.json\n",
      "============================================="
     ]
    }
   ],
   "source": [
    "NUM_TARGET_FILES = 100\n",
    "if TESTMODE:\n",
    "    shard = json.load(open('example_merged.json'))\n",
    "    generate_sessions_by_user(shard, 'sessions_by_user/', NUM_TARGET_FILES)\n",
    "else:\n",
    "    merged_data_prefix = 'gs://ma-muy/baseline_dataset/merged/'\n",
    "    sessions_by_user_prefix = 'gs://ma-muy/baseline_dataset/sessions_by_user/'\n",
    "    temp_sessions_by_user_prefix = 'temp_sessions_by_user/'\n",
    "    \n",
    "    client = storage.Client.from_service_account_json('../../service-account.json')\n",
    "    \n",
    "    merged_paths = get_paths_with_prefix(merged_data_prefix)\n",
    "    for merged_path in merged_paths[-2:]:\n",
    "        \n",
    "        print('Downloading {}'.format(merged_path))\n",
    "        source = dict_ops.load_dict(merged_path)\n",
    "        \n",
    "        print('Processing {}'.format(merged_path))\n",
    "        generate_sessions_by_user(source, temp_sessions_by_user_prefix, NUM_TARGET_FILES)\n",
    "    \n",
    "    temp_files = get_paths_with_prefix(temp_sessions_by_user_prefix)\n",
    "    for temp_file in temp_files:\n",
    "        file_name = temp_file.rsplit('/', 1)[1]\n",
    "        if 'ipynb' not in file_name:\n",
    "            target_uri = sessions_by_user_prefix + file_name\n",
    "            print('Uploading {} to {}'.format(temp_file, target_uri))\n",
    "            copy_file(temp_file, target_uri, gcs_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate User Parallel Mini batches\n",
    "\n",
    "- Now that we know all the sessions of all the users we can generate the user parallel mini batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "MIN_EVENTS_PER_SESSION = 5\n",
    "\n",
    "if TESTMODE:\n",
    "    sessions_by_user_prefix = 'sessions_by_user/'\n",
    "else:\n",
    "    sessions_by_user_prefix = 'gs://ma-muy/baseline_dataset/sessions_by_user/'\n",
    "iterator = user_parallel_batch_iterator(BATCH_SIZE, sessions_by_user_prefix, MIN_EVENTS_PER_SESSION)\n",
    "batches = []\n",
    "for idx, batch in enumerate(iterator):\n",
    "    batches.append(batch)\n",
    "    if idx == 200:\n",
    "        break\n",
    "pprint.PrettyPrinter(width=240, compact=True).pprint(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have created the user parallel mini batches\n",
    "- This notebook can be split into two parts:\n",
    "    - First we have the export of the data and transformation into sessions by users\n",
    "    - Second we have the generation of the mini batches based on the sessions by users\n",
    "    \n",
    "- The first part should be implemented in a ETL Pipeline and will be executed daily\n",
    "- The second part is part of the dataset implementation inside the model repository"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
