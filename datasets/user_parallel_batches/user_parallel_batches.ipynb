{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Dataset\n",
    "\n",
    "- This dataset is used to implement the baseline from the paper \"Personalizing Session based Recommendations with Hierarchical RNNs\" -> resources/papers/personalizing_session_based_rec.pdf\n",
    "- This dataset is generated from the OnlineShopTrafficTracking Table in BigQuery\n",
    "- Clean out bots using: https://github.com/monperrus/crawler-user-agents/blob/master/crawler-user-agents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "TESTMODE = True\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ResourceWarning)\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import StringIO\n",
    "from dg_ml_core.datastores import gcs_utils\n",
    "from dg_ml_core.file import get_file_handle, get_paths_with_prefix, save_to_file, file_exists, copy_file\n",
    "from dg_ml_core.collections import dict_ops\n",
    "from dg_ml_core.datastores import gcs_utils\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bots_list():\n",
    "    url = 'https://raw.githubusercontent.com/monperrus/crawler-user-agents/master/crawler-user-agents.json'\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.content)\n",
    "    all_instances = [item for sublist in map(lambda x: x['instances'], data) for item in sublist]\n",
    "    return all_instances\n",
    "\n",
    "def clean_dataset(source, target):\n",
    "    col_types = {\"ProductId\": int, \n",
    "             \"UserId\": int, \n",
    "             \"UserAgent\": str, \n",
    "             \"LastLoggedInUserId\": int, \n",
    "             \"SessionId\": int, \n",
    "             \"Timestamp\": int}\n",
    "    \n",
    "    df = pd.read_csv(source).fillna(-1).astype(col_types)\n",
    "    bots = get_bots_list()\n",
    "    df = df[~df.UserAgent.isin(bots)]\n",
    "    \n",
    "    no_user_id_mask = df.UserId == -1\n",
    "    df.loc[no_user_id_mask, 'UserId'] = df.loc[no_user_id_mask, 'LastLoggedInUserId']\n",
    "    \n",
    "    df.to_csv(target, index=False, columns=['UserId', 'ProductId', 'SessionId', 'Timestamp'])\n",
    "\n",
    "    return target\n",
    "\n",
    "def merge_sessions(reader):\n",
    "    sessions_by_user = dict()\n",
    "    for row in reader:\n",
    "        if row['UserId'] not in sessions_by_user:\n",
    "            sessions_by_user[row['UserId']] = dict()\n",
    "        if row['SessionId'] not in sessions_by_user[row['UserId']]:\n",
    "            sessions_by_user[row['UserId']][row['SessionId']] = dict()\n",
    "            sessions_by_user[row['UserId']][row['SessionId']]['Events'] = []\n",
    "        \n",
    "        sessions_by_user[row['UserId']][row['SessionId']]['Events'].append(\n",
    "            {\n",
    "                \"ProductId\": int(row['ProductId']),\n",
    "                \"Timestamp\": int(row['Timestamp'])\n",
    "            })\n",
    "        \n",
    "        first_event_ts = min(map(lambda x: int(x['Timestamp']), sessions_by_user[row['UserId']][row['SessionId']]['Events']))\n",
    "        sessions_by_user[row['UserId']][row['SessionId']]['StartTime'] = first_event_ts\n",
    "    return sessions_by_user\n",
    "\n",
    "def generate_sessions_by_user(shard, merged_shards_prefix):\n",
    "    \n",
    "    gcs_client = gcs_utils.get_client('machinelearning-prod', None)\n",
    "    \n",
    "    path = ''\n",
    "    new_path = ''\n",
    "    output_dict = dict()\n",
    "    \n",
    "    for user_id in shard:\n",
    "        if int(user_id) > 0:\n",
    "            new_path = merged_shards_prefix + str(int(user_id) % 100) + '.json' # Add a datestamp hierarchy\n",
    "\n",
    "            if new_path != path:\n",
    "                if path != '':\n",
    "                    dict_ops.save_dict(path, output_dict, gcs_client=gcs_client)\n",
    "                path = new_path\n",
    "\n",
    "                if file_exists(path):\n",
    "                    output_dict = dict_ops.load_dict(path, gcs_client=gcs_client)\n",
    "                else:\n",
    "                    output_dict = dict()\n",
    "\n",
    "            for session_id in shard[user_id]:\n",
    "                if user_id not in output_dict:\n",
    "                    output_dict[user_id] = dict()\n",
    "\n",
    "                if session_id not in output_dict[user_id]:\n",
    "                    output_dict[user_id][session_id] = shard[user_id][session_id]\n",
    "\n",
    "                else:\n",
    "                    merged_events = output_dict[user_id][session_id]['Events'] + shard[user_id][session_id]['Events']\n",
    "                    merged_events_str = map(lambda x: json.dumps(x), merged_events)\n",
    "                    unique_events_str = set(merged_events_str)\n",
    "                    unique_events = list(map(lambda x: json.loads(x), unique_events_str))\n",
    "                    output_dict[user_id][session_id]['Events'] = unique_events\n",
    "                    output_dict[user_id][session_id]['StartTime'] = min(map(lambda x: int(x['Timestamp']), unique_events))\n",
    "    \n",
    "    dict_ops.save_dict(new_path, output_dict, gcs_client=gcs_client)\n",
    "\n",
    "def user_iterator(sessions_by_user_prefix):\n",
    "    paths = get_paths_with_prefix(sessions_by_user_prefix)\n",
    "    for path in paths:\n",
    "        merged_shard = dict_ops.load_dict(path)\n",
    "        user_ids = list(merged_shard.keys())\n",
    "        random.shuffle(user_ids)\n",
    "        for user_id in user_ids:\n",
    "            yield user_id, merged_shard[user_id]\n",
    "\n",
    "def event_iterator(user_sessions, min_events_per_session):\n",
    "    sorted_sessions = sorted(map(lambda x: user_sessions[x], user_sessions.keys()), key=lambda y: y['StartTime'])\n",
    "    for sorted_session in sorted_sessions:\n",
    "        if len(sorted_session['Events']) < min_events_per_session:\n",
    "            continue\n",
    "            \n",
    "        sorted_events = sorted(sorted_session['Events'], key=lambda z: z['Timestamp'])\n",
    "        for event in sorted_events:\n",
    "            yield event['ProductId']\n",
    "        \n",
    "        yield '<EOS>'\n",
    "        \n",
    "def get_next_event_or_none(active_user):\n",
    "    try:\n",
    "        return next(active_user['Events'])\n",
    "    except StopIteration:\n",
    "        return None\n",
    "    \n",
    "def get_next_user_or_none(users, min_events_per_session):\n",
    "    try:\n",
    "        user_id, user_sessions = next(users)\n",
    "        return {\n",
    "            'UserId': int(user_id),\n",
    "            'Events': event_iterator(user_sessions, min_events_per_session)\n",
    "        }\n",
    "    except StopIteration:\n",
    "        return None\n",
    "        \n",
    "def user_parallel_batch_iterator(batch_size, sessions_by_user_prefix, min_events_per_session):\n",
    "\n",
    "        active_users = dict()\n",
    "        users = user_iterator(sessions_by_user_prefix)\n",
    "    \n",
    "        data = [[]]*batch_size\n",
    "\n",
    "        # Initial fill of users\n",
    "        for i in range(batch_size):\n",
    "            active_users[i] = get_next_user_or_none(users, min_events_per_session)\n",
    "        \n",
    "        while True:\n",
    "            next_batch = dict()\n",
    "            for idx in active_users:\n",
    "                if active_users[idx] is None:\n",
    "                    next_batch[idx] = ('<EOU>', '<EOS>')\n",
    "                    continue\n",
    "                next_event = get_next_event_or_none(active_users[idx])\n",
    "                while next_event is None:\n",
    "                    next_user = get_next_user_or_none(users, min_events_per_session)\n",
    "                    if next_user is None:\n",
    "                        print('There are no more new users')\n",
    "                        active_users[idx] = None\n",
    "                        break\n",
    "                    else:\n",
    "                        active_users[idx] = next_user\n",
    "                        next_event = get_next_event_or_none(active_users[idx])\n",
    "                else:\n",
    "                    next_batch[idx] = (active_users[idx]['UserId'], next_event)\n",
    "            if len(set(next_batch.values())) == 1:\n",
    "                return\n",
    "            yield list(next_batch.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Query in BQ\n",
    "\n",
    "- Here we extract the relevant features out of the large collection of visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query \n",
      "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
      "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
      "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show'\n",
      " AND _PARTITIONTIME = TIMESTAMP(\"2019-02-11\"). \n",
      "You have 5 seconds to cancel...\n",
      "Running Job baseline_dataset_query_46a37433-e183-435b-aa6b-ca40170d95ab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7fe32584ee48>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution done\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
    "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
    "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show' AND UserId > 0\n",
    "\"\"\"\n",
    "\n",
    "if TESTMODE:\n",
    "    query += ' AND _PARTITIONTIME = TIMESTAMP(\"2019-02-11\")'\n",
    "    \n",
    "print('Executing query {}. \\nYou have 5 seconds to cancel...'.format(query))\n",
    "time.sleep(5)\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('baseline_dataset')\n",
    "\n",
    "job_config = bigquery.job.QueryJobConfig(\n",
    "    allow_large_results=True, \n",
    "    destination=table_ref,\n",
    "    write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE)\n",
    "\n",
    "query_job = client.query(query, job_config=job_config, job_id_prefix='baseline_dataset_query_', location='EU')\n",
    "print('Running Job {}'.format(query_job.job_id))\n",
    "query_job.result()\n",
    "\n",
    "print('Query execution done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract to GCS\n",
    "\n",
    "- Extract the table containing the relevant features to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Job baseline_dataset_extract_78ed9046-82b1-4de6-9707-8cbcec1225e0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7fe3268db2b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction done\n"
     ]
    }
   ],
   "source": [
    "destination_uri = 'gs://ma-muy/baseline_dataset/raw/*.csv'\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('baseline_dataset')\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    location='EU',\n",
    "    job_id_prefix='baseline_dataset_extract_')\n",
    "\n",
    "print('Running Job {}'.format(extract_job.job_id))\n",
    "extract_job.result()\n",
    "print('Extraction done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "- Here we clean the data.\n",
    "- Specifically there are two steps:\n",
    "  - Clean out bot visits\n",
    "  - Merge LastLoggedInUserId and UserId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000000.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000000.csv\n",
      "Downloading gs://ma-muy/baseline_dataset/raw/000000000001.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/clean/000000000001.csv\n"
     ]
    }
   ],
   "source": [
    "if TESTMODE:\n",
    "    print('Processing example.csv')\n",
    "    df = clean_dataset('example.csv', 'example_clean.csv')\n",
    "    \n",
    "else:\n",
    "    raw_data_prefix = 'gs://ma-muy/baseline_dataset/raw/'\n",
    "    cleaned_data_prefix = 'gs://ma-muy/baseline_dataset/clean/'\n",
    "        \n",
    "    raw_paths = get_paths_with_prefix(raw_data_prefix)\n",
    "    \n",
    "    for raw_path in raw_paths:\n",
    "        clean_path = cleaned_data_prefix + gcs_utils.get_file_name(raw_path)\n",
    "\n",
    "        print('Downloading {}'.format(raw_path))\n",
    "        source = get_file_handle(raw_path)\n",
    "        target = StringIO()\n",
    "        \n",
    "        target = clean_dataset(source, target)\n",
    "        \n",
    "        print('Uploading {}'.format(clean_path))\n",
    "        save_to_file(clean_path, target.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sessions\n",
    "\n",
    "- In this step we will merge all the single visit events into sessions\n",
    "- Further we merge all sessions to the specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000000.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000000.json\n",
      "Downloading gs://ma-muy/baseline_dataset/clean/000000000001.csv\n",
      "Uploading gs://ma-muy/baseline_dataset/merged/000000000001.json\n"
     ]
    }
   ],
   "source": [
    "if TESTMODE:\n",
    "    reader = csv.DictReader(open('example_clean.csv'))\n",
    "    \n",
    "    sessions_by_user = merge_sessions(reader)\n",
    "    \n",
    "    dict_ops.save_dict('example_merged.json', sessions_by_user)\n",
    "\n",
    "else:\n",
    "    cleaned_data_prefix = 'gs://ma-muy/baseline_dataset/clean/'\n",
    "    merged_data_prefix = 'gs://ma-muy/baseline_dataset/merged/'\n",
    "    \n",
    "    clean_paths = get_paths_with_prefix(cleaned_data_prefix)\n",
    "    for clean_path in clean_paths:\n",
    "        merged_path = (merged_data_prefix + gcs_utils.get_file_name(clean_path)).replace('csv', 'json')\n",
    "        \n",
    "        print('Downloading {}'.format(clean_path))\n",
    "        source = get_file_handle(clean_path)\n",
    "        reader = csv.DictReader(source)\n",
    "        \n",
    "        sessions_by_user = merge_sessions(reader)\n",
    "        \n",
    "        print('Uploading {}'.format(merged_path))\n",
    "        dict_ops.save_dict(merged_path, sessions_by_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge shards\n",
    "\n",
    "- As of now we have several shards, containing the sessions aggregated to the user level.\n",
    "- The merging of the shards is the most time consuming part of the data generation process. \n",
    "- We need to merge all sessions of a specific user into one datastructure.\n",
    "- In production we will be dealing with daily shards, which makes the generation of the dataset easier\n",
    "- However in this case we will be dealing with full exports, therefore we cannot assume that a shard is from one day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://ma-muy/baseline_dataset/merged/000000000000.json\n",
      "Downloading gs://ma-muy/baseline_dataset/merged/000000000001.json\n",
      "Uploading temp_sessions_by_user/75.json to gs://ma-muy/baseline_dataset/sessions_by_user/75.json\n",
      "Uploading temp_sessions_by_user/46.json to gs://ma-muy/baseline_dataset/sessions_by_user/46.json\n",
      "Uploading temp_sessions_by_user/83.json to gs://ma-muy/baseline_dataset/sessions_by_user/83.json\n",
      "Uploading temp_sessions_by_user/71.json to gs://ma-muy/baseline_dataset/sessions_by_user/71.json\n",
      "Uploading temp_sessions_by_user/9.json to gs://ma-muy/baseline_dataset/sessions_by_user/9.json\n",
      "Uploading temp_sessions_by_user/55.json to gs://ma-muy/baseline_dataset/sessions_by_user/55.json\n",
      "Uploading temp_sessions_by_user/4.json to gs://ma-muy/baseline_dataset/sessions_by_user/4.json\n",
      "Uploading temp_sessions_by_user/68.json to gs://ma-muy/baseline_dataset/sessions_by_user/68.json\n",
      "Uploading temp_sessions_by_user/28.json to gs://ma-muy/baseline_dataset/sessions_by_user/28.json\n",
      "Uploading temp_sessions_by_user/61.json to gs://ma-muy/baseline_dataset/sessions_by_user/61.json\n",
      "Uploading temp_sessions_by_user/63.json to gs://ma-muy/baseline_dataset/sessions_by_user/63.json\n",
      "Uploading temp_sessions_by_user/89.json to gs://ma-muy/baseline_dataset/sessions_by_user/89.json\n",
      "Uploading temp_sessions_by_user/79.json to gs://ma-muy/baseline_dataset/sessions_by_user/79.json\n",
      "Uploading temp_sessions_by_user/52.json to gs://ma-muy/baseline_dataset/sessions_by_user/52.json\n",
      "Uploading temp_sessions_by_user/20.json to gs://ma-muy/baseline_dataset/sessions_by_user/20.json\n",
      "Uploading temp_sessions_by_user/18.json to gs://ma-muy/baseline_dataset/sessions_by_user/18.json\n",
      "Uploading temp_sessions_by_user/44.json to gs://ma-muy/baseline_dataset/sessions_by_user/44.json\n",
      "Uploading temp_sessions_by_user/24.json to gs://ma-muy/baseline_dataset/sessions_by_user/24.json\n",
      "Uploading temp_sessions_by_user/48.json to gs://ma-muy/baseline_dataset/sessions_by_user/48.json\n",
      "Uploading temp_sessions_by_user/51.json to gs://ma-muy/baseline_dataset/sessions_by_user/51.json\n",
      "Uploading temp_sessions_by_user/82.json to gs://ma-muy/baseline_dataset/sessions_by_user/82.json\n",
      "Uploading temp_sessions_by_user/66.json to gs://ma-muy/baseline_dataset/sessions_by_user/66.json\n",
      "Uploading temp_sessions_by_user/27.json to gs://ma-muy/baseline_dataset/sessions_by_user/27.json\n",
      "Uploading temp_sessions_by_user/91.json to gs://ma-muy/baseline_dataset/sessions_by_user/91.json\n",
      "Uploading temp_sessions_by_user/30.json to gs://ma-muy/baseline_dataset/sessions_by_user/30.json\n",
      "Uploading temp_sessions_by_user/8.json to gs://ma-muy/baseline_dataset/sessions_by_user/8.json\n",
      "Uploading temp_sessions_by_user/37.json to gs://ma-muy/baseline_dataset/sessions_by_user/37.json\n",
      "Uploading temp_sessions_by_user/62.json to gs://ma-muy/baseline_dataset/sessions_by_user/62.json\n",
      "Uploading temp_sessions_by_user/16.json to gs://ma-muy/baseline_dataset/sessions_by_user/16.json\n",
      "Uploading temp_sessions_by_user/95.json to gs://ma-muy/baseline_dataset/sessions_by_user/95.json\n",
      "Uploading temp_sessions_by_user/77.json to gs://ma-muy/baseline_dataset/sessions_by_user/77.json\n",
      "Uploading temp_sessions_by_user/41.json to gs://ma-muy/baseline_dataset/sessions_by_user/41.json\n",
      "Uploading temp_sessions_by_user/38.json to gs://ma-muy/baseline_dataset/sessions_by_user/38.json\n",
      "Uploading temp_sessions_by_user/73.json to gs://ma-muy/baseline_dataset/sessions_by_user/73.json\n",
      "Uploading temp_sessions_by_user/90.json to gs://ma-muy/baseline_dataset/sessions_by_user/90.json\n",
      "Uploading temp_sessions_by_user/36.json to gs://ma-muy/baseline_dataset/sessions_by_user/36.json\n",
      "Uploading temp_sessions_by_user/43.json to gs://ma-muy/baseline_dataset/sessions_by_user/43.json\n",
      "Uploading temp_sessions_by_user/65.json to gs://ma-muy/baseline_dataset/sessions_by_user/65.json\n",
      "Uploading temp_sessions_by_user/47.json to gs://ma-muy/baseline_dataset/sessions_by_user/47.json\n",
      "Uploading temp_sessions_by_user/87.json to gs://ma-muy/baseline_dataset/sessions_by_user/87.json\n",
      "Uploading temp_sessions_by_user/22.json to gs://ma-muy/baseline_dataset/sessions_by_user/22.json\n",
      "Uploading temp_sessions_by_user/88.json to gs://ma-muy/baseline_dataset/sessions_by_user/88.json\n",
      "Uploading temp_sessions_by_user/40.json to gs://ma-muy/baseline_dataset/sessions_by_user/40.json\n",
      "Uploading temp_sessions_by_user/67.json to gs://ma-muy/baseline_dataset/sessions_by_user/67.json\n",
      "Uploading temp_sessions_by_user/96.json to gs://ma-muy/baseline_dataset/sessions_by_user/96.json\n",
      "Uploading temp_sessions_by_user/3.json to gs://ma-muy/baseline_dataset/sessions_by_user/3.json\n",
      "Uploading temp_sessions_by_user/78.json to gs://ma-muy/baseline_dataset/sessions_by_user/78.json\n",
      "Uploading temp_sessions_by_user/15.json to gs://ma-muy/baseline_dataset/sessions_by_user/15.json\n",
      "Uploading temp_sessions_by_user/98.json to gs://ma-muy/baseline_dataset/sessions_by_user/98.json\n",
      "Uploading temp_sessions_by_user/35.json to gs://ma-muy/baseline_dataset/sessions_by_user/35.json\n",
      "Uploading temp_sessions_by_user/14.json to gs://ma-muy/baseline_dataset/sessions_by_user/14.json\n",
      "Uploading temp_sessions_by_user/34.json to gs://ma-muy/baseline_dataset/sessions_by_user/34.json\n",
      "Uploading temp_sessions_by_user/31.json to gs://ma-muy/baseline_dataset/sessions_by_user/31.json\n",
      "Uploading temp_sessions_by_user/53.json to gs://ma-muy/baseline_dataset/sessions_by_user/53.json\n",
      "Uploading temp_sessions_by_user/19.json to gs://ma-muy/baseline_dataset/sessions_by_user/19.json\n",
      "Uploading temp_sessions_by_user/6.json to gs://ma-muy/baseline_dataset/sessions_by_user/6.json\n",
      "Uploading temp_sessions_by_user/86.json to gs://ma-muy/baseline_dataset/sessions_by_user/86.json\n",
      "Uploading temp_sessions_by_user/69.json to gs://ma-muy/baseline_dataset/sessions_by_user/69.json\n",
      "Uploading temp_sessions_by_user/11.json to gs://ma-muy/baseline_dataset/sessions_by_user/11.json\n",
      "Uploading temp_sessions_by_user/97.json to gs://ma-muy/baseline_dataset/sessions_by_user/97.json\n",
      "Uploading temp_sessions_by_user/29.json to gs://ma-muy/baseline_dataset/sessions_by_user/29.json\n",
      "Uploading temp_sessions_by_user/33.json to gs://ma-muy/baseline_dataset/sessions_by_user/33.json\n",
      "Uploading temp_sessions_by_user/80.json to gs://ma-muy/baseline_dataset/sessions_by_user/80.json\n",
      "Uploading temp_sessions_by_user/54.json to gs://ma-muy/baseline_dataset/sessions_by_user/54.json\n",
      "Uploading temp_sessions_by_user/56.json to gs://ma-muy/baseline_dataset/sessions_by_user/56.json\n",
      "Uploading temp_sessions_by_user/17.json to gs://ma-muy/baseline_dataset/sessions_by_user/17.json\n",
      "Uploading temp_sessions_by_user/45.json to gs://ma-muy/baseline_dataset/sessions_by_user/45.json\n",
      "Uploading temp_sessions_by_user/85.json to gs://ma-muy/baseline_dataset/sessions_by_user/85.json\n",
      "Uploading temp_sessions_by_user/49.json to gs://ma-muy/baseline_dataset/sessions_by_user/49.json\n",
      "Uploading temp_sessions_by_user/13.json to gs://ma-muy/baseline_dataset/sessions_by_user/13.json\n",
      "Uploading temp_sessions_by_user/42.json to gs://ma-muy/baseline_dataset/sessions_by_user/42.json\n",
      "Uploading temp_sessions_by_user/26.json to gs://ma-muy/baseline_dataset/sessions_by_user/26.json\n",
      "Uploading temp_sessions_by_user/76.json to gs://ma-muy/baseline_dataset/sessions_by_user/76.json\n",
      "Uploading temp_sessions_by_user/64.json to gs://ma-muy/baseline_dataset/sessions_by_user/64.json\n",
      "Uploading temp_sessions_by_user/72.json to gs://ma-muy/baseline_dataset/sessions_by_user/72.json\n",
      "Uploading temp_sessions_by_user/60.json to gs://ma-muy/baseline_dataset/sessions_by_user/60.json\n",
      "Uploading temp_sessions_by_user/59.json to gs://ma-muy/baseline_dataset/sessions_by_user/59.json\n",
      "Uploading temp_sessions_by_user/25.json to gs://ma-muy/baseline_dataset/sessions_by_user/25.json\n",
      "Uploading temp_sessions_by_user/10.json to gs://ma-muy/baseline_dataset/sessions_by_user/10.json\n",
      "Uploading temp_sessions_by_user/0.json to gs://ma-muy/baseline_dataset/sessions_by_user/0.json\n",
      "Uploading temp_sessions_by_user/21.json to gs://ma-muy/baseline_dataset/sessions_by_user/21.json\n",
      "Uploading temp_sessions_by_user/57.json to gs://ma-muy/baseline_dataset/sessions_by_user/57.json\n",
      "Uploading temp_sessions_by_user/23.json to gs://ma-muy/baseline_dataset/sessions_by_user/23.json\n",
      "Uploading temp_sessions_by_user/7.json to gs://ma-muy/baseline_dataset/sessions_by_user/7.json\n",
      "Uploading temp_sessions_by_user/84.json to gs://ma-muy/baseline_dataset/sessions_by_user/84.json\n",
      "Uploading temp_sessions_by_user/2.json to gs://ma-muy/baseline_dataset/sessions_by_user/2.json\n",
      "Uploading temp_sessions_by_user/99.json to gs://ma-muy/baseline_dataset/sessions_by_user/99.json\n",
      "Uploading temp_sessions_by_user/12.json to gs://ma-muy/baseline_dataset/sessions_by_user/12.json\n",
      "Uploading temp_sessions_by_user/93.json to gs://ma-muy/baseline_dataset/sessions_by_user/93.json\n",
      "Uploading temp_sessions_by_user/94.json to gs://ma-muy/baseline_dataset/sessions_by_user/94.json\n",
      "Uploading temp_sessions_by_user/5.json to gs://ma-muy/baseline_dataset/sessions_by_user/5.json\n",
      "Uploading temp_sessions_by_user/81.json to gs://ma-muy/baseline_dataset/sessions_by_user/81.json\n",
      "Uploading temp_sessions_by_user/70.json to gs://ma-muy/baseline_dataset/sessions_by_user/70.json\n",
      "Uploading temp_sessions_by_user/74.json to gs://ma-muy/baseline_dataset/sessions_by_user/74.json\n",
      "Uploading temp_sessions_by_user/92.json to gs://ma-muy/baseline_dataset/sessions_by_user/92.json\n",
      "Uploading temp_sessions_by_user/39.json to gs://ma-muy/baseline_dataset/sessions_by_user/39.json\n",
      "Uploading temp_sessions_by_user/1.json to gs://ma-muy/baseline_dataset/sessions_by_user/1.json\n",
      "Uploading temp_sessions_by_user/50.json to gs://ma-muy/baseline_dataset/sessions_by_user/50.json\n",
      "Uploading temp_sessions_by_user/58.json to gs://ma-muy/baseline_dataset/sessions_by_user/58.json\n",
      "Uploading temp_sessions_by_user/32.json to gs://ma-muy/baseline_dataset/sessions_by_user/32.json\n"
     ]
    }
   ],
   "source": [
    "TESTMODE=False\n",
    "if TESTMODE:\n",
    "    shard = json.load(open('example_merged.json'))\n",
    "    generate_sessions_by_user(shard, 'sessions_by_user/')\n",
    "else:\n",
    "    merged_data_prefix = 'gs://ma-muy/baseline_dataset/merged/'\n",
    "    sessions_by_user_prefix = 'gs://ma-muy/baseline_dataset/sessions_by_user/'\n",
    "    temp_sessions_by_user_prefix = 'temp_sessions_by_user/'\n",
    "    \n",
    "    merged_paths = get_paths_with_prefix(merged_data_prefix)\n",
    "    for merged_path in merged_paths:\n",
    "        \n",
    "        print('Downloading {}'.format(merged_path))\n",
    "        source = dict_ops.load_dict(merged_path)\n",
    "        \n",
    "        generate_sessions_by_user(source, temp_sessions_by_user_prefix)\n",
    "    \n",
    "    temp_files = get_paths_with_prefix(temp_sessions_by_user_prefix)\n",
    "    for temp_file in temp_files:\n",
    "        file_name = temp_file.rsplit('/', 1)[1]\n",
    "        target_uri = sessions_by_user_prefix + file_name\n",
    "        print('Uploading {} to {}'.format(temp_file, target_uri))\n",
    "        copy_file(temp_file, target_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate User Parallel Mini batches\n",
    "\n",
    "- Now that we know all the sessions of all the users we can generate the user parallel mini batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "MIN_EVENTS_PER_SESSION = 5\n",
    "\n",
    "if TESTMODE:\n",
    "    sessions_by_user_prefix = 'sessions_by_user/'\n",
    "else:\n",
    "    sessions_by_user_prefix = 'gs://ma-muy/baseline_dataset/sessions_by_user/'\n",
    "iterator = user_parallel_batch_iterator(BATCH_SIZE, sessions_by_user_prefix, MIN_EVENTS_PER_SESSION)\n",
    "batches = []\n",
    "for idx, batch in enumerate(iterator):\n",
    "    batches.append(batch)\n",
    "    if idx == 200:\n",
    "        break\n",
    "pprint.PrettyPrinter(width=240, compact=True).pprint(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have created the user parallel mini batches\n",
    "- This notebook can be split into two parts:\n",
    "    - First we have the export of the data and transformation into sessions by users\n",
    "    - Second we have the generation of the mini batches based on the sessions by users\n",
    "    \n",
    "- The first part should be implemented in a ETL Pipeline and will be executed daily\n",
    "- The second part is part of the dataset implementation inside the model repository"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_base",
   "language": "python",
   "name": "thesis_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
