{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Dataset\n",
    "\n",
    "- This dataset is used to implement the baseline from the paper \"Personalizing Session based Recommendations with Hierarchical RNNs\" -> resources/papers/personalizing_session_based_rec.pdf\n",
    "- This dataset is generated from the OnlineShopTrafficTracking Table in BigQuery\n",
    "- Clean out bots using: https://github.com/monperrus/crawler-user-agents/blob/master/crawler-user-agents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "TESTMODE = True\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=ResourceWarning)\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import StringIO\n",
    "from dg_ml_core.datastores import gcs_utils\n",
    "import requests\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bots_list():\n",
    "    url = 'https://raw.githubusercontent.com/monperrus/crawler-user-agents/master/crawler-user-agents.json'\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.content)\n",
    "    all_instances = [item for sublist in map(lambda x: x['instances'], data) for item in sublist]\n",
    "    return all_instances\n",
    "\n",
    "def clean_dataset(source, target):\n",
    "    col_types = {\"ProductId\": int, \n",
    "             \"UserId\": int, \n",
    "             \"UserAgent\": str, \n",
    "             \"LastLoggedInUserId\": int, \n",
    "             \"SessionId\": int, \n",
    "             \"Timestamp\": int}\n",
    "    \n",
    "    df = pd.read_csv(source).fillna(-1).astype(col_types)\n",
    "    bots = get_bots_list()\n",
    "    df = df[~df.UserAgent.isin(bots)]\n",
    "    \n",
    "    no_user_id_mask = df.UserId == -1\n",
    "    df.loc[no_user_id_mask, 'UserId'] = df.loc[no_user_id_mask, 'LastLoggedInUserId']\n",
    "    \n",
    "    df.to_csv(target, index=False, columns=['UserId', 'ProductId', 'SessionId', 'Timestamp'])\n",
    "\n",
    "    return target\n",
    "\n",
    "def merge_sessions(reader):\n",
    "    sessions_by_user = dict()\n",
    "    for row in reader:\n",
    "        if row['UserId'] not in sessions_by_user:\n",
    "            sessions_by_user[row['UserId']] = dict()\n",
    "        if row['SessionId'] not in sessions_by_user[row['UserId']]:\n",
    "            sessions_by_user[row['UserId']][row['SessionId']] = dict()\n",
    "            sessions_by_user[row['UserId']][row['SessionId']]['Events'] = []\n",
    "        \n",
    "        sessions_by_user[row['UserId']][row['SessionId']]['Events'].append(\n",
    "            {\n",
    "                \"ProductId\": int(row['ProductId']),\n",
    "                \"Timestamp\": int(row['Timestamp'])\n",
    "            })\n",
    "        \n",
    "        first_event_ts = min(map(lambda x: int(x['Timestamp']), sessions_by_user[row['UserId']][row['SessionId']]['Events']))\n",
    "        sessions_by_user[row['UserId']][row['SessionId']]['StartTime'] = first_event_ts\n",
    "    return sessions_by_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Query in BQ\n",
    "\n",
    "- Here we extract the relevant features out of the large collection of visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query \n",
      "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
      "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
      "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show'\n",
      " AND _PARTITIONTIME = TIMESTAMP(\"2019-02-11\"). \n",
      "You have 5 seconds to cancel...\n",
      "Running Job baseline_dataset_query_46a37433-e183-435b-aa6b-ca40170d95ab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7fe32584ee48>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution done\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT (SELECT Value FROM UNNEST(ActionParameters) WHERE Key = 'id') as ProductId, LastLoggedInUserId, UserId, SessionId, UserAgent, Timestamp\n",
    "FROM `dg-prod-personalization.PersonalizationDataV2.OnlineShopTrafficTracking` \n",
    "WHERE LOWER(ControllerName) = 'product' AND LOWER(ActionName) = 'show'\n",
    "\"\"\"\n",
    "\n",
    "if TESTMODE:\n",
    "    query += ' AND _PARTITIONTIME = TIMESTAMP(\"2019-02-11\")'\n",
    "    \n",
    "print('Executing query {}. \\nYou have 5 seconds to cancel...'.format(query))\n",
    "time.sleep(5)\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('baseline_dataset')\n",
    "\n",
    "job_config = bigquery.job.QueryJobConfig(\n",
    "    allow_large_results=True, \n",
    "    destination=table_ref,\n",
    "    write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE)\n",
    "\n",
    "query_job = client.query(query, job_config=job_config, job_id_prefix='baseline_dataset_query_', location='EU')\n",
    "print('Running Job {}'.format(query_job.job_id))\n",
    "query_job.result()\n",
    "\n",
    "print('Query execution done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract to GCS\n",
    "\n",
    "- Extract the table containing the relevant features to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Job baseline_dataset_extract_78ed9046-82b1-4de6-9707-8cbcec1225e0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.job.ExtractJob at 0x7fe3268db2b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction done\n"
     ]
    }
   ],
   "source": [
    "destination_uri = 'gs://ma-muy/baseline_dataset/raw/*.csv'\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('MAMuy', project='machinelearning-prod')\n",
    "table_ref = dataset_ref.table('baseline_dataset')\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    location='EU',\n",
    "    job_id_prefix='baseline_dataset_extract_')\n",
    "\n",
    "print('Running Job {}'.format(extract_job.job_id))\n",
    "extract_job.result()\n",
    "print('Extraction done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "- Here we clean the data.\n",
    "- Specifically there are two steps:\n",
    "  - Clean out bot visits\n",
    "  - Merge LastLoggedInUserId and UserId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTMODE:\n",
    "    print('Processing example.csv')\n",
    "    df = clean_dataset('example.csv', 'example_clean.csv')\n",
    "    \n",
    "else:\n",
    "    raw_data_prefix = 'gs://ma-muy/baseline_dataset/raw/'\n",
    "    cleaned_data_prefix = 'gs://ma-muy/baseline_dataset/clean/'\n",
    "    \n",
    "    client = storage.Client()\n",
    "    \n",
    "    raw_uris = gcs_utils.get_uris_with_prefix(raw_data_prefix, storage_client=client)\n",
    "    \n",
    "    for raw_uri in raw_uris:\n",
    "        clean_uri = cleaned_data_prefix + gcs_utils.get_file_name(raw_uri)\n",
    "\n",
    "        print('Downloading {}'.format(raw_uri))\n",
    "        source = StringIO(gcs_utils.download_string(raw_uri))\n",
    "        target = StringIO()\n",
    "        \n",
    "        clean_dataset(source, target)\n",
    "        \n",
    "        print('Uploading {}'.format(clean_uri))\n",
    "        gcs_utils.upload_string(target.getvalue(), clean_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sessions\n",
    "\n",
    "- In this step we will merge all the single visit events into sessions\n",
    "- Further we merge all sessions to the specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if TESTMODE:\n",
    "    reader = csv.DictReader(open('example_clean.csv'))\n",
    "    \n",
    "    sessions_by_user = merge_sessions(reader)\n",
    "    \n",
    "    json.dump(sessions_by_user, open('example_merged.json', 'w'), indent=2)\n",
    "\n",
    "else:\n",
    "    cleaned_data_prefix = 'gs://ma-muy/baseline_dataset/clean/'\n",
    "    merged_data_prefix = 'gs://ma-muy/baseline_dataset/merged/'\n",
    "    \n",
    "    clean_uris = gcs_utils.get_uris_with_prefix(cleaned_data_prefix)\n",
    "    for clean_uri in clean_uris:\n",
    "        merged_uri = merged_data_prefix + gcs_utils.get_file_name(clean_uri)\n",
    "        \n",
    "        print('Downloading {}'.format(clean_uri))\n",
    "        source = StringIO(gcs_utils.download_string(clean_uri))\n",
    "        reader = csv.DictReader(source)\n",
    "        \n",
    "        sessions_by_user = merge_sessions(reader)\n",
    "        \n",
    "        target = StringIO()\n",
    "        json.dump(sessions_by_user, target, indent=2)\n",
    "        \n",
    "        print('Uploading {}'.format(merged_uri))\n",
    "        gcs_utils.upload_string(target.getvalue(), merged_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge shards\n",
    "\n",
    "- As of now we have several shards for each day, containing the sessions aggregated to the user level.\n",
    "- The merging of the shards is the most time consuming part of the data generation process. \n",
    "- We need to merge all sessions of a specific user into one datastructure.\n",
    "- However since we work with daily exports we can stop aggregate the data only over one day, this saves a lot of time and the data generation process can be iteratively extended day by day.\n",
    "- For now we will not track sessions that go over multiple days, we approximate that by ending the session at midnight and starting a new one the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate User Parallel Mini batches\n",
    "\n",
    "- Now that we know all the sessions of all the users we can generate the user parallel mini batches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_base",
   "language": "python",
   "name": "thesis_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
